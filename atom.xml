<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>math.h</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://5imcs.com/"/>
  <updated>2020-07-29T23:12:04.086Z</updated>
  <id>https://5imcs.com/</id>
  
  <author>
    <name>Ben</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>强化学习论文(multi-agent RL综述篇)</title>
    <link href="https://5imcs.com/posts/5ad812f1/"/>
    <id>https://5imcs.com/posts/5ad812f1/</id>
    <published>2020-07-26T16:00:00.000Z</published>
    <updated>2020-07-29T23:12:04.086Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>从综述开始,我觉得比较好的两篇: </p></blockquote><h4 id="1-例子比较简单-可以实验下-Multi-agent-reinforcement-learning-MARL-An-overview-2010"><a href="#1-例子比较简单-可以实验下-Multi-agent-reinforcement-learning-MARL-An-overview-2010" class="headerlink" title="[1]:例子比较简单,可以实验下. Multi-agent reinforcement learning(MARL): An overview,2010"></a>[1]:例子比较简单,可以实验下. Multi-agent reinforcement learning(MARL): An overview,2010</h4><p>多智能体系统由一群自主,可交互的实体组成,它们共享一个环境,并由传感器感知环境,执行器采取行动.这一系统在各种邻域已有应用,如机器人团队,分布式控制,资源管理,协作决策系统collaborative decision support system,数据挖掘等.尽管可以对智能体赋予预设行为,但因为环境的复杂性,在随时间变化的环境中,先前良好的行为可能会变得不合适或者难以实施,所以它们经常需要在线学习新的行为,从而使智能体或整个多智能体性能提升[115,106].</p><p>强化学习rl通过与动态环境交互来进行学习.在每个时间步长,agent会感知环境状态,并采取行动,从而转到一个新的状态.设计奖励函数对其评估,agent必须在交互过程中将奖励累积最大化.强化学习反馈的信息量比监督学习少,但多于无监督学习[104].单智能体任务的rl算法比较成熟(有易于理解,且经证明收敛的算法).对设定进行一些简化或者泛化,使得rl也可以用于多智能体中.</p><p>本文全面概述了marl,主要关注自主agent使用rl(temporal-difference rl)算法在线学习解决动态任务.并讨论了博弈论对marl(及静态任务的重要算法)的贡献.我们首先指出marl的优势及挑战.实际上该领域的关键在于为multi-agent system定义恰当的目标.我们介绍了文献中提出的不同学习目标,一方面考虑了系统的稳定性,另一方面也考虑了agent对其它agent不断变化行为的适应性.本章的核心包括对marl代表性算法详细概述.</p><p>按任务类型组织算法:fully cooperative完全合作(fcoo), fully competitive完全竞争(fcom),以及mixed;然后根据学习目标的类型确定目标:稳定性,适应性或两者结合.此外,我们简要讨论marl技术应用领域,并在一个模拟实例中进行比较.该实例设计两个协作agent运输目标物体.在展望中,我们提出一些未解决的问题及有希望的方向.</p><p>本文其余部分安排如下,sec2介绍了单agent的rl,marl,博弈论的必要背景.sec3回顾了marl的主要优缺点.sec4介绍了有代表性marl算法目标.<font color=red>sec5对marl进行了分类</font>.sec6回顾了几个应用领域,sec7提供了一个实例(运输目标物体).sec8总结了marl前景.sec8介绍了相关工作,sec10总结.</p><h5 id="sec2-RL背景"><a href="#sec2-RL背景" class="headerlink" title="sec2:RL背景"></a>sec2:RL背景</h5><p>本节介绍了但智能体,多智能体rl的相关背景.首先,介绍single-agent学习任务及求解算法.然后过渡到多智能体的强化学习marl,讨论仅限于具有有限数量的离散状态和动作空间.实际上大部分marl算法也是在这设定下提出的.</p><h6 id="the-single-agent-sa-case-可以参考以前一篇笔记-读论文continuous-control-with-deep-reinforcement-learning并测试walking-robot学习实例-math-h"><a href="#the-single-agent-sa-case-可以参考以前一篇笔记-读论文continuous-control-with-deep-reinforcement-learning并测试walking-robot学习实例-math-h" class="headerlink" title="the single-agent(sa) case(可以参考以前一篇笔记:读论文continuous control with deep reinforcement learning并测试walking robot学习实例 | math.h)"></a>the single-agent(sa) case(可以参考以前一篇笔记:<a href="https://5imcs.com/posts/1080553f/">读论文continuous control with deep reinforcement learning并测试walking robot学习实例 | math.h</a>)</h6><p>sarl可以用markov 决策过程表示,定义如下:<br>(def1). 有限markov决策过程是一个4元组$&lt;X,U,f,\rho&gt;$ ,$X$表示agent的状态空间,$U$是动作空间,$f:X\times U\times X\rightarrow [0,1]$是状态转移概率函数,<br>$\rho:X\times U\times X\rightarrow R$是奖励函数.</p><p>$x_k\in X$描述每个离散时间步$k$的状态,agent观察状态并采取一个动作$u_k$.作为结果,agent状态根据状态转移函数$f(x_k,u_k,x_{k+1})$转移到了下一个状态.agent再由奖励函数$\rho(x_k,u_k,x_{k+1})$获得奖励$r_{k+1}\in R$.该奖励对动作$u_k$进行评估.但从表达式看,它并没有说明这一行动长期效果.我们假设奖励有界.</p><p>对于确定性系统,转移概率函数变成$f’:X\times U\rightarrow X$.此时即时奖励仅与当前状态和动作有关,即$r_{k+1} = \rho’(x_k,u_k)$.某些markov决策过程具有最终状态(吸收状态?).从最终状态获得奖励为0.在这种情况下,学习过程通常分为不同的回合(<font color=red>就像那个寻找宝藏的例子</font>).</p><p>agent在任一状态下如何选择决策的过程称为agent的policy策略.策略可以是随机的$h:X\times U\rightarrow [0,1]$,也可以是确定性的$h’:X\rightarrow U$.agent的目标就是找到一种策略,可以从每个状态$x_k$,最大化期望折扣收益:</p><p>$R^h(x)=E[\sum\gamma^kr_{K+1}|x_0 =x,h]$</p><p>其中$\gamma\in[0,1]$为折扣因子,刻画了未来收益的不确定性,目的是保证从长远来看,收益$R$代表了奖励的累积.然而agent每次只能获得当前步的即时奖励.将其转换成通过计算最佳 state-action value 函数(q-函数).$Q^h:X\times U\rightarrow R$给出策略$h$从任何状态-动作中获得的预期收益.</p><p>$Q^h(x,u)=E[\sum\gamma^kr_{K+1}|x_0 =x,u_o=u,h]$</p><p>最优$Q$函数定义为$Q^{*}(x,u)=max_hQ^h(x,u)$.并满足bellman方程:</p><p>$Q^{<em>}(x,u)=\sum_{x’\in X}f(x,u,x’)[\rho(x,u,x’)+\gamma max_{u’}Q^{</em>}(x’,u’)],\forall x\in X,u\in U$</p><p>等式表明在状态$x$下采取动作$u$的最优回报是期望的即时回报，加上下一状态的最优(折扣)回报值.当$Q^{*}$计算好后,最优策略就是在每一状态下,选择回报最大的动作.</p><p>$h’^{<em>}(x)=arg \max_uQ^{</em>}(x,u)$</p><p>当多个动作达到最大$Q$值时,可以选择任意一个,并且保持策略最优.在这种情况下,<code>arg</code><br>运算符被解释为仅返回有效解决方案(u)之一.以这种方式最大化Q的策略被认为是贪婪的,因此可以首先确定$Q^{<em>}$,然后计算$Q^{</em>}$中的贪婪策略来找到最优策略.</p><p>已经存在很多rl算法,比如基于在线估计q函数的model-free方法[6,89],model-based方法(通常称为动态规划,<font color=red>dynamics programming</font>)[8,96].以及model-learning方法(估计模型,然后使用model-based技术)[79,119].the model包括转移概率和奖励函数.许多marl都是从q-learning[137]的model-free算法得出的,例如[17,42,49,67,69,70].</p><p>q-learning是一种将bellman方程迭代近似求解的方法:</p><p>$Q_{k+1}(x_k,u_k)=Q_k(x_k,u_k)+\alpha [r_{k+1}+\gamma max_{u’}Q_k(x_{k+1},u’)-Q_k(x_k,u_k)]$</p><p>算法先随机初始化$Q(x,u)$值,然后不停迭代更新.方括号之间的是temporal difference.即当前$Q_k(x_k,u_k)$估计与更新估计$r_{k+1}+\gamma max_{u’}Q_k(x_{k+1},u’)$间的差.这个新的估计是bellman的右侧的一种迭代版本.在这里,$x’$由下一状态$x_{k+1}$替换,$\rho(x_k,u_k,x’)$被奖励$r_{K+1}$替换.学习率$\alpha$随时间变化,一般降低.</p><p>在以下条件下,序列$Q_k$证明收敛于$Q^{*}$:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- explicit,distinct values of Q-function被存储并且对于每个状态-动作对更新</span><br><span class="line">- $sum \alpha_k^2$ is finite,while  $sum \alpha_k$ infinite</span><br><span class="line">- all the state-action paris are visited infinitely often.</span><br></pre></td></tr></table></figure><p>除此之外,agent一直以非零概率在所以状态下尝试所有动作,这称为探索.例如在每个步骤中随机以$e\in (0,1)$中选择随机动作,$(1-e)$选择贪婪动作.就得到了$e-$贪婪探索过程.当然还可以使用boltzmann探索过程,agent在状态$x$下依概率$h(x,u)$选择动作,定义如下:</p><p>$h(x,u)=\frac{e^{Q(x,u)/\tau} }{\sum_{u’}e^{Q(x,u’)/\tau} }$</p><p>$\tau$为超函数,当$\tau$接近0,agent倾向选择最优动作,当其无限大,agent随机.</p><h6 id="the-multi-agent-case"><a href="#the-multi-agent-case" class="headerlink" title="the multi-agent case"></a>the multi-agent case</h6><p>markov决策过程泛化到ma情形则是随机博弈.</p><p>(def2)随机博弈是一个多元组$&lt;X,U_1,…,U_n,f,\rho_1,…,\rho_n&gt;$,n为agent个数,$X$是状态集合,$U_i$为其对应的动作空间,所有agent动作空间为$U=U_1\times…\times U_n$.$f:X\times U\times X\rightarrow [0,1]$是状态转移函数.$\rho_i:X\times U\times X\rightarrow R,i=1,..,n$是每个agent奖励函数.</p><p>依旧假定奖励函数有界.在ma情形下,转移函数是每个agent联合动作的结果,$u_k=[u^T_{1,k},…,u^T_{n,k}]^T,u_k\in U,u_{i,k}\in U_i$.每个agent的策略$h_i:X\times U_i\rightarrow [0,1]$,所有agent的策略组成了联合策略joint policy $h$.因为奖励函数$r_{i,k+1}$取决于联合行动,所以其回报也取决于joint policy:</p><p>$R^h_i(x) =  E{ \sum \gamma^kr_{i,k+1}|x_0=x,h }$</p><p>Q函数(depends on joint action and on the joint policy)</p><p>$Q^h_i(x,u) = E{ \sum\gamma^kr_{i,k+1}|x_0=x,u_0=u,h }$</p><p>在完全合作的随机博弈张,所有agent的奖励函数相同:$\rho_1=..\rho_n$.因为所有agent具有相同的目标:最大化收益.假设$n=2$,且$\rho_1=-\rho_2$,两个agent具有相反的目标,随机博弈完全竞争的.混合博弈mixed game是除上述之外的博弈.</p><h6 id="static-repeated-and-stage-games-待研究"><a href="#static-repeated-and-stage-games-待研究" class="headerlink" title="static, repeated,and stage games(待研究)"></a>static, repeated,and stage games(<font color=red>待研究</font>)</h6><p>许多marl算法是为static game或者以stage-wise fashion方式work设计的.接下来介绍有关static games的博弈理论[3,39].</p><p>static game 是一种无状态信号且没有动力学的随机博弈.用元组$&lt;U_1,…,U_n,\rho_1,…,\rho_n&gt;$表示,奖励仅取决于联合动作$\rho_i:U\rightarrow R$.当只有两个agent,该博弈称为<font color=blue>bimatrix 博弈</font>.因为两个agent中奖励函数可表示为$|U_1|\times|U_2|$,其行row对应agent1的动作,列对应agent2的动作,,$|\cdot|$表示基数.完全竞争的静态博弈也称为零和博弈.因为奖励矩阵之和为0.混合静态博弈一般也称为一般博弈,因为agent奖励和没有限制.</p><p>stage game是随机博弈在特定状态下出现的静态博弈.其奖励函数为投影在联合动作空间的随机博弈的q函数,通常,agent会多次访问随机博弈相同状态,所以stage game是重复game.在博弈理论中,重复博弈是由相同agent反复进行的静态博弈,与 one-shot 博弈主要区别在于,agent可以使用一些game iteration来收集关于其它agent行为或奖励信息,并在此做出更明智的选择.</p><p>在静态或repeated game中,policy丢失状态参数,于是变为策略$\sigma_i: U_i\in [0,1]$.当状态为固定值,agent在随机博弈中某些状态下产生stage博弈策略为其策略$h_i$在动作空间$U_i$中的投影.marl算法在每个stage game分别依赖于 stage-wise approach学习策略.agent的整体策略为这些策略的和.</p><p>静态博弈的重要解决方法是纳什平衡,首先,将agent i的最佳相应定义为对手策略opponent strategies的向量,作为可获得最大预期奖励的策略$\sigma^{*}_i$.</p><p>$E{r_i|\sigma_1,…,\sigma_n}\leq E{r_i|\sigma_1,.,\sigma_i^{*},..,\sigma_n}$</p><p>纳什平衡是一种联合策略$[\sigma^{*}_i]^T$,这样每个agent的策略都是对其它策略的最佳相应.纳什平衡描述了一种状态,只要其它所有agent保持策略不变,任何agent都不会从改变其策略中受到益.任何静态博弈中至少有一个(可能随机的)纳什平衡.一些静态博弈有多重纳什平衡.许多marl算法力求收敛到纳什平衡.</p><p>随机策略在marl中非常重要,因其表达某些概念(如纳什平衡)是必不可少的.</p><h5 id="sec3-机会与挑战-学习目标-不稳定性-及need-for-coordinate"><a href="#sec3-机会与挑战-学习目标-不稳定性-及need-for-coordinate" class="headerlink" title="sec3:机会与挑战(学习目标,不稳定性,及need for coordinate)"></a>sec3:机会与挑战(学习目标,不稳定性,及need for coordinate)</h5><p>marl较sarl存在一定优势,比如marl的分布式性质(可通过并行计算实现加速),此外,多个rl agent可以通过交流,教学或模仿等分享经验,更快更好完成任务,当系统中有agent失效,其它agent可以代替执行任务,从而使得系统更加鲁棒,也可方便加入新的agent,扩展性好.此外也面临一些挑战,包括维度灾难问题更加严重,转移概率函数和奖励函数都是在联合动作空间下计算,随着状态和动作维数增加,计算复杂性指数增长.</p><p>再者就是学习的目标不好定义,agent的奖励于其它agent的行为相关,没办法单独最大化某个agent的奖励.一些文献提出了marl目标几种类型,考虑了agent学习动力学的稳定性[50],对其它agent的行为变化的适应性[93],或两者兼顾[14,16,17,26,70].参见sec4.</p><p>不稳定性也是marl的一个问题.agent是同时在学习的,每个agent都面临一个不断变化的环境,最好的策略也可能会随着其它agent策略的改变而改变.最后,探索和贪婪过程会更加复杂,在多agent下,探索不仅是为了获取环境信息,还包括其它agent的信息,以此来适应其它agent的行为,但又不能过度探索从而打破其它agent平衡.</p><p>need for coordinate源于以下事实,任何agent动作对环境影响还取决于其它agent采取的动作.因此,agent的动作必须选择相互一致才能达能预期效果(<font color=blue>?</font>),协同(coordinate)通常可以归结为在同样良好联合动作或策略之间始终如一的联系.在协作环境cooperative setting中需要协同coordinate.例如,考虑许多国家将电力电网互联,各国家地区由agent管理,尽管每个agent目标都是优化本国能源利益,但它仍需要协调邻国之间的电力流coordinate on the power flow,以实现最有意义的方案[84].</p><h5 id="sec4-marl-goal"><a href="#sec4-marl-goal" class="headerlink" title="sec4:marl goal"></a>sec4:marl goal</h5><p>在完全合作fcoo的随机博弈中,共同收益可以一起最大化.但在其它情况下,agent回报通常是不同并相关的.无法独自最大化,指定一个好的目标是一个难题.</p><p>主要从:稳定性(收敛到固定策略),适应性(保证agent改变策略时的性能),及两者兼顾讨论.</p><p>目标通常根据策略及奖励来指定静态博弈的条件.通过要求一些动态博弈中所有状态逐步满足条件,可将某些目标拓展到动态博弈.在这种情况下,目标是根据阶段策略和预期回报来指定的(不再是策略和奖励.)</p><p>收敛到平衡是稳定性基本要求[42,50].这意味着agent的策略最终收敛到coordinate equilibrium.纳什平衡是最常用的,但是,人们对其实用性提出了关注[108].例如,阶段性收到纳什平衡与动态博弈中的性能之间联系尚不清楚.</p><p>在[16,17]中,为了保持稳定性需要收敛,并添加了合理性作为适应标准.为了算法收敛,作者要求学习者收敛于平稳策略,假设其它agent使用预定义的目标类算法,合理性在文中作为目标收敛到最佳相应的要求,尽管不明确要求到纳什平衡,但如果所有agent都是理性且收敛的,则自然会出现.</p><p>rationality另一种选择是no-regret,见[14].要求agent获得的收益至少与任何固定策略的收益一样好,这适用于其它agent的任一组策略.这可以防止学习者被其它agent利用.注意,对于一些静态博弈,无悔学习算法收敛于纳什平衡[54,143].</p><p>有针对的最优性/兼容性/安全性是 适应性要求,以平均奖励的有界形式表示[93].针对目标的最优性需要需要平均回报,至少是最佳相应的平均回报.兼容性规定了在self-play中平均回报的级别.即当其他agent使用学习算法.安全性要求针对所有的算法的安全级别的平均回报,满足这些要求的算法不一定收敛于平稳策略.</p><p>marl算法其它属性也可能与稳定性和适应性有关,例如,与对手无关的学习,反之与是适应性有关[15,70].对手无关opponet-independent算法收敛于策略.与对手有关的算法学习其它agent模型,并以某种形式的最佳相应对它们做出反应.table1总结了marl算法的要求与属性.</p><p>stability and adaptation in MARL(<font color=blue>没看明白这小节..估计得从博弈理论入手吧</font>)</p><p><img src="5.png" alt=""></p><h5 id="sec5-marl-algorithms"><a href="#sec5-marl-algorithms" class="headerlink" title="sec5:marl algorithms"></a>sec5:marl algorithms</h5><p>方法比较久了.粗略看一下:</p><p>MARL算法基于任务类型分类,主要有fully cooperative, fully competitive,以及mixed.</p><p>(也可以通过算法侧重的学习目标来分)marl算法表现出其它agent的意识程度与agent追求的学习目标密切相关.关注稳定性的算法通常对其它agent是独立无感知的.在考虑适应性的算法需对其它agent行为有一定程度上的感知.如极端情况下忽略稳定性问题,则算法tracking跟踪其它agent的行为.即使没有明确的稳定性或适应性目标,算法也能展现出智能体的意识程度.所有agent tracking的算法和agent-aware的算法都用了对手模型来tracking其它agent的策略[25,49,133].</p><p>此外还有一些其它的分类,包括</p><ul><li><p>homogeneity of rl,仅当所有agent的使用该算法(team-q,nash-q),或其它agent可以使用其它学习算法(awesome wolf-phc)</p></li><li><p>关于任务先验知识的假设,任务模型是否用于agent(model-based,)或者(model-free, team-Q).该模型包括过渡函数transition function(除非博弈是静态的)以及agent的奖励函数</p></li><li><p>关于agent输入的假设,通常假定输入精确的环境状态.不同体现在agent需要观察其它agent的行为,(如,team-q); 行为和回报(nash-q),或者两者都不(wolf-phc)</p></li></ul><p><img src="6.png" alt=""></p><p>Marl可以看作是 temporal difference RL(尤其是q学习)和博弈论,以及更一般策略搜索技术的融合.</p><p><img src="1.png" alt=""></p><h6 id="fully-cooperative"><a href="#fully-cooperative" class="headerlink" title="fully cooperative"></a><font color=red>fully cooperative</font></h6><p>对于完全合作的marl算法,如果在随机博弈中,会存在一个中心控制者(centralized controller),可以控制其它agent的动作.那么就能求解最大的联合动作值,随机博弈退化称markov决策过程,并且可以用q-learning来求解.</p><p>$Q_{k+1}(x_k,u_k)=Q_k(x_k,u_k)+\alpha [r_{k+1}+\gamma max_{u’}Q_k(x_{k+1},u’)-Q_k(x_k,u_k)]$ (7)</p><p>并且使用贪婪策略greedy policy,但是agent是独立决策者,即使所有agent都使用上述方程学习最优Q函数,也会出现coordination协同问题.agent应用$Q^{\star}$的贪婪策略来最大化回报:(每个agent都选择当前状态下最优的动作)</p><p>$h’<em>i(x) = arg \max</em>{u_1,..u_{i-1},u_{i+1},…u_n}Q^{\star}(x,u)$ (8)</p><p>但是在某些情况下,联合动作才能取得最优(此时单agent不一定最优).在没有coordination协同机制下,不同agent或许会打破最优联合动作的联系.</p><p><img src="2.png" alt=""></p><p>(例子1)比方说.如图,两个移动agent在保持队形maintaining formation(仅是相对距离保持)时需要避障,每个agent有三个动作,go sraight$s_i$, left($l_i$),right($r_i$).</p><p>对于给定状态(agent位置),可以将Q函数投影到projected到联合动作空间.对于上图的所有状态,映射到q表.这个表描述了完全合作(协作)的静态(阶段 stage)博弈.行表示agent1的动作,列对应agent2的动作.如果两个agent left,或者同时right,那么有机会躲过障碍.Q 值为10.如果一个往左一个往右,那么破坏了队形,尽管仍有机会通过障碍,这时回报为0.其它情况例子发生碰撞q为负.</p><p>Coordination-free methods</p><ul><li><p>Team Q学习,通过<b>假设最佳联合动作唯一</b>来避免协同问题avoids the coordination Problem,然后所有agent并行学习q函数,使用(8)选择最优联合动作来使得收益最大化.</p></li><li><p>在没有负回报的确定性问题中,可以用distribute q-learning算法求解,不需要协同机制,每个agent有布局策略$h’_i(x)$和一个取决自身动作的局部Q函数.当更新导致Q值增加时,才会更新局部q值.</p></li></ul><p>$Q_{i,k+1}(x_k,u_{i,k})=max{Q_{i,k}(x_k,u_{i,k}), r_{k+1}+\gamma max_{u’<em>i}Q</em>{i,k}(x_{k+1},u’_i)}$</p><p>这样可以保证局部q值使用拥有联合动作q值最大值.$Q_{i,k}(x,u_i)= max <em>{u_1,…,u</em>{i-1}},u_{i+1},..u_n Q_{k}(x,u), \forall k$,其中$u= [u_1,…,u_n]^T$,$u_i$固定.当仅更新导致q值有所改善,才会更新本地策略.这样可以保证联合策略$[h_{i,k}]^T$相对全局$Q_k$最优.并且在$Q_{i,0}=0$条件下,agent局部策略收敛的最优联合策略.</p><p>coordination-based methods</p><p>例如,通信multi-agent decision 问题[97].</p><p><b>indirect coordination methods 间接协同方法</b></p><p>该方法使得动作偏向于可能会带来丰厚回报的动作.这使得agent趋于协同动作选择(?).使用诸如 其它agent估计模型,或过去观察到的奖励的统计,来评估良好回报的可能性.</p><p>(比如平均场方法?)</p><p>Joint action learners[29]算法中,agent $i$为其它agent建模:</p><p>$\sigma_j^i(u_j) = \frac{C^i_j(u_j) }{\sum_{u’_j\in U_j}C^i_j(u’_j)}$</p><p>其中$\sigma_j^i$是agent $i$对$j$的策略模型,$C^i_j(u_j)$统计了$i$观察$j$采取动作$u_j$的次数.结合这些模型,衍生出几个启发式方法,来提高state-action的回报,</p><p>比如FMQ方法,统计每个动作取得最优回报的频率,然后利用这个值对Q函数进行修改:</p><p>公式(12)</p><p>最后是基于社会公约等显式的协同机制.约定每个agent的先后顺序以及动作选择的先后,这些信息是共享的.回到上面那个例子,规定a1优先a2,而且动作优先选择l,那么a1在行动时查表得知往左或右都有最高收益,根据社会公约,采取一个方向l1.轮到a2时,根据公约推理a1是选择l1,那么a2就采取l2,从而实现最大化的收益.</p><h6 id="fully-competitive-tasks-页19"><a href="#fully-competitive-tasks-页19" class="headerlink" title="fully competitive tasks(页19)"></a><font color=red>fully competitive tasks(页19)</font></h6><p>略</p><p>参见 <a href="http://www.algorithmdog.com/multi-agent-rl" target="_blank" rel="noopener">http://www.algorithmdog.com/multi-agent-rl</a> .</p><p>大致思路是在完全竞争博弈中,应用最小最大化minimax原则,假定对手会采取使自己收益最小动作的情况,采取使自己最大收益的动作,使用公式来更新a1的策略函数和q函数.</p><p>以例子说明:</p><p><img src="3.png" alt=""></p><p>a1需要达到x位置,同时避免a2抓到.a2的目标就是抓捕a1.两个agent只能左右动作.图右侧是a1的q表,两个通水往左不产生收益,同时往右则a1达成目标且没有被抓到,收益10,根据minimax原则,a1应该选择往左走,以往往左最小收益期望为0,对于a2,其最优策略是往左保护目标位置.但如果a2不是采取最优策略,且a1通过建模可以预测到,那么a1往右走,达成目标.</p><h6 id="mixed-tasks-页20"><a href="#mixed-tasks-页20" class="headerlink" title="mixed tasks(页20)"></a><font color=red>mixed tasks(页20)</font></h6><p>在混合任务中大部分算法针对静态任务,而且关注适应性.最简单的就是直接应用单agent的rl算法,即每个agent的q函数只和自己有关,对对手没有感知.[130]在如何理解单agent在ma task中工作迈出了重要一步,文章将结果应用到进化博弈论(evolutionary game theory)中,用boltzmann策略在repeated games中分析q学习的学习动态.在某些情况下,q学习可以收敛到coordinated平衡点,而在其它情况下会存在不稳定的循环振荡.</p><p>例子:<br><img src="4.png" alt=""></p><p>一个equilibrium selection问题.考虑如图,两个清洁机器人1,2到达建筑物一个交界处,每个机器人需要清洁建筑物侧翼中的任一个.如果两个agent清理一个侧翼,效率会很低.两个agent都喜欢清理较小的那个,因为省力气.对于这种情况,表格给出了agent q函数在联合动作空间的投影.涉及博弈,如果两个agent都选择相同侧翼,则无法有效清洁建筑物.对于这些回报,存在两个确定的纳什平衡,(l1,r2)和(r1,l2).如果打破这一联系,从而导致次优的联合动作,这是均衡的选择问题,与完全协作任务中出现的协同问题相对应,需要配合协调机制比如社会公约来解决这一问题.</p><p>dynamic task. win-or-learn-fast policy hill-climbing [17]是一种启发式算法,通过q学习规则更新q函数.定义了两种策略,即当前策略$h(s,a)$和平均策略$h’(s,a’)$.当前策略是一种概率分布函数,初始值为$h(s,a)=\frac{1}{|A_i|}$.这个概率分布函数当agent选择动作a时进行更新,更新方法是对于q函数,最好的动作即$a= max arg_aQ(s,a’)$,则增加概率,其它动作则降低概率.wolf-phc会不断更新平均策略,并与当前策略进行比较:如果当前策略平均奖励大于平均策略的奖励,即$\sum_ah(s,a)Q(s,a)&gt;\sum_ah’(s,a)Q(s,a)$,则认为agent是wining的,此时平均策略采用$\sigma_{win}$速度来更新策略,否则则认为当前agent是losing的,用较大的$\sigma_{lose}$来更快的自适应学习.(<font color=blue>摘录了一下,没看懂</font>)</p><h5 id="sec6-应用领域"><a href="#sec6-应用领域" class="headerlink" title="sec6:应用领域"></a>sec6:应用领域</h5><p>multi-robot teams:也称作是多机器人系统,是marl受欢迎应用领域之一.机器人环境是真实或模拟的空间,通常是二维.机器人使用marl掌握各项技能,从导航等基本行为到踢足球等复杂行为.在导航中,每个机器人到达目标位置的同时避免障碍及与其它agent的干扰[17,50].</p><p>多目标观察multi-target observation是探索任务的扩展,机器人必须将一组运动目标保持传感器范围内[35.128].追捕包括由一组agent捕获运动目标,在一个流行版本,几个捕食者agent必须通过converging on it来捕获猎物[52,60].运输要求将一组对象搬运到最终位置,其中一些物体质量不一,可能超过一个机器人的运输能力,因此需要多个机器人进行协同控制.sec7实例属于此类.</p><p>足球机器人是marl流行的复杂测试平台.需要上面的大多数技能[77,114,131,142].例如,拦截球并将其带入球门设计对象的retrieval和运输技巧,而球员在球场上的战术部署是coverage任务的高级版本.</p><h5 id="sec7-coordinated-multi-agent-object-transportation"><a href="#sec7-coordinated-multi-agent-object-transportation" class="headerlink" title="sec7: coordinated multi-agent object transportation"></a>sec7: coordinated multi-agent object transportation</h5><p>任务描述:两个agent协同coordinated运输对象.agent在$7\times 6$的二维网格上移动.agent必须在最短时间内将目标对象object运输到home base.同时避障.</p><p><img src="7.png" alt=""></p><p>agent初始位置如图.并且在每个时间步中可以向左,右或者上\下移动一个单元格.也可以静止不动.如果单元格非空,则agent不移动.如果两个agent试图移动到同一个单元格,则两个agent都不移动.为了运输目标,agent必须先抓住它.当agent达到目标左侧或者右侧的单元格,它会自动抓取目标.一旦抓住了目标就 无法释放(<font color=red>这时候编队保持了吧~</font>).只有agent在同一方向上协同拉动时,目标才移动.目标达到home base,终止,<br>并将agent和目标重新初始化进行新的实验.</p><p>描述每个agent $i$的状态变量有两位置坐标,$p_{i,X}\in {1,…,7},p_{i,Y}\in {1,..,6}$.用一个变量记录agent抓取目标的状态$g_i\in {free,grasping-left,grasp-right}$.因为,RL完整的状态向量为$s = [p_{1,X},p_{1,Y},g_1,p_{2,X},p_{2,Y},g_2]^T$.The grasping<br>variables are needed to ensure that the state vector has the Markov property.<br>动作空间是$U_i={left,right,up,down,stand-still}$.并不是所有状态都有效,一些碰撞可能会阻止某些组合的发生.</p><p>任务是fully cooperative的,每个agent有相同奖励函数,表示为抓住物体为1,达到base为10,其余为0.</p><p>agent面临两个coordinated问题,一是要确定哪个agent先通过狭窄下部通道,二是要确定围绕障碍物哪一侧?</p><p>用三个算法来解决这个问题.包括<b>单Agent算法,team Q-learning以及WoLF-PHC</b>.<br>设定折扣因子$\gamma=0.8,$学习率$\alpha=0.1$.</p><p>论文给出了三种算法在100次独立运行中的平均学习性能.以及平均值0.95的置信区间.从图中可以看到,一般20-30次实验(回合)后,算法就可以收敛到良好的性能.值得注意的是q-learning表现非常好,尽管单agent对其它agent无感知unaware.team-q和wolfphc算法中agent将其它agent考虑在内,但并没有通信,而是实现了一种隐式的协同,:偶尔学习到了一个很好的方案,从而忽视其它方案.不需要明确协同的事实可以在添加了社交约定的team-q学习实验验证.在这个问题上,single-agent q学习是可取的,因为它提供了相同的性能,并且更加简单.</p><p>下图给出了team-q算法的轨迹,agent 1通过障碍口,抓住物体左边,原地等待.agent 2通过障碍口,抓住右侧,然后一起越过上方障碍物,到达目的地.</p><p><img src="8.png" alt=""></p><h5 id="sec8-outlook"><a href="#sec8-outlook" class="headerlink" title="sec8: outlook"></a>sec8: outlook</h5><p>本节讨论marl中的一般性开放问题,涉及marl算法在实践中应用的适应性,ma学习目标的选择以及对联合环境和学习动力学learning dynamics的研究.</p><h6 id="practical-marl"><a href="#practical-marl" class="headerlink" title="practical marl"></a>practical marl</h6><p>(2010年),marl算法通常应用与小问题,比如说static games,及小网格世界.所以,不清楚是否可以拓展到现实物理的ma问题,对于这些问题,状态和动作空间很大甚至是连续的.很少算法能够处理不完整,不确定的观察结果.然后marl对实际问题的实用性是必不可少的研究.我们列举了几个方向,并指出一些开创性工作.</p><p>可拓展性scalability是marl的主要关注点,多数算法是要对agent q函数及其策略进行显式的表格存储.这意味它们仅在离散状态和动作相对较少的问题中起作用.当状态和动作空间包含大量或无限元素(连续时),q函数表格存储变得不切实际甚至不可能,取而代之的是,q函数必须近似表示,已经提出了近似的marl算法(<font color=blue>作者很有远见,实际上后面的dqn算法就是为了解决q表问题</font>).例如用于离散大状态动作空间[1],以及用于连续状态和离散动作[35,122].不幸的是,这些算法许多仅在有限问题类别中起作用.</p><h6 id="joint-environment-and-learning-dynamics"><a href="#joint-environment-and-learning-dynamics" class="headerlink" title="joint environment and learning dynamics"></a>joint environment and learning dynamics</h6><p>目前为止,基于博弈理论的分析仅应用于agent的learning dynamics[130,132].尚未明确考虑学习动力学.整个学习过程中,由鲁棒性控制开发的工具可以扮演重要角色.其中包括相互作用的环境和learning dynamics.此外,该框架可以结合不完善的观测边界值和先验知识,例如噪声损坏的变量,有助于研究marl算法针对其它agent dynamics不确定性的鲁棒性.</p><hr><h4 id="2-A-Survey-and-Critique-of-Multiagent-Deep-Reinforcement-Learning-2019"><a href="#2-A-Survey-and-Critique-of-Multiagent-Deep-Reinforcement-Learning-2019" class="headerlink" title="[2]: A Survey and Critique of Multiagent Deep Reinforcement Learning,2019;"></a>[2]: A Survey and Critique of Multiagent Deep Reinforcement Learning,2019;</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;从综述开始,我觉得比较好的两篇: &lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;1-例子比较简单-可以实验下-Multi-agent-reinforcement-learning-MARL-An-overview-2010&quot;&gt;&lt;a
      
    
    </summary>
    
    
      <category term="查文献" scheme="https://5imcs.com/categories/%E6%9F%A5%E6%96%87%E7%8C%AE/"/>
    
    
      <category term="查文献" scheme="https://5imcs.com/tags/%E6%9F%A5%E6%96%87%E7%8C%AE/"/>
    
  </entry>
  
  <entry>
    <title>机器人学,机器视觉与控制-MATLAB算法基础(动力学与控制篇)</title>
    <link href="https://5imcs.com/posts/49286280/"/>
    <id>https://5imcs.com/posts/49286280/</id>
    <published>2020-07-11T16:00:00.000Z</published>
    <updated>2020-07-25T10:54:05.911Z</updated>
    
    <content type="html"><![CDATA[<h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><ul><li>讨论串联机械臂的动力学与控制,在simulink上测试机器人三环控制,参考书为robotics,vision,and control,第二版.</li></ul><h4 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h4><ul><li>动力学方程</li></ul><h4 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h4><h5 id="independent-joint-control-独立关节控制"><a href="#independent-joint-control-独立关节控制" class="headerlink" title="independent joint control (独立关节控制)"></a>independent joint control (独立关节控制)</h5><p>机器人传动系统包括驱动器(电机)以及连接至杆的传动装置. 机器人关节控制的一种常见方法是将每个关节视作一个独立的控制系统, 使其精确跟随各自关节角度轨迹.由于作用在关节上重力,速度及加速度的耦合,摩擦力产生的各种干扰使得情况变得复杂. 所以, <font color=red>嵌套</font>的控制回路是一种常见的控制结构.<br>其outer loop(外环)负责保持关节位置(p),并确定使位置误差最小的关节速度(velocity of joint);inner loop(内环)负责保持外环所需的关节速度.</p><h6 id="Actuators"><a href="#Actuators" class="headerlink" title="Actuators"></a>Actuators</h6><p>如今多数机器人都是旋转电机(rotary electric motors)驱动的.大型工业机器人通常使用无刷伺服电机(brushless servo motors).而实验室多数使用直流有刷电机(brushed DC motors)或步进电机(stepper motors).采矿,林业或建筑业中大负载机械手通常使用液压驱动(electro-hydraulic).</p><p><img src="1.png" alt=""></p><p>电压$u$控制流入电机的电流$i_m$,产生转矩$\tau_m$,加速电机惯量$J_m$,其相对摩擦力记为$B_m\omega_m$.编码器测量转角及转速$\theta,\omega$.上图再接入齿轮箱,对于一个$G:1$的减速传动,连杆处扭矩应是电机扭矩$G$倍.</p><p>电机可以由电流或电压控制.我们假设由电机驱动或放大器(amplifier)提供电流. $i_m = K_a u$. 其中$K_a$为transconductance of amplifier,单位为$(AV^{-1})$ .电机产生的转矩正比与电流: $\tau_m = K_m i_M$.其中$K_m$为电机转矩系数,$(NmA^{-1})$.</p><h6 id="Friction"><a href="#Friction" class="headerlink" title="Friction"></a>Friction</h6><p>任何旋转的机械,电机或齿轮箱(gearbox)都受到摩擦力影响.电机净转矩为 $\tau’ = \tau_m- \tau_f$.其中$\tau_f = B\omega + \tau_C$. 其中$B&gt;0$为粘性摩擦系数(viscous friction coefficient), 偏移量为库伦摩擦力.后者由非线性函数建模. $\tau_C =  0, \omega = 0$,$\tau_C =  \tau^+_C , \omega &gt; 0$. </p><p>摩擦力系数取决与旋转方向,库伦摩擦的不对称性比粘性摩擦更明显.</p><p><img src="2.png" alt=""></p><p>在低速情况下(灰色区域),粘滞现象(stiction effect)明显.机器移动后,静摩擦迅速减小,粘性摩擦起主导作用.</p><h6 id="Effect-of-the-link-mass"><a href="#Effect-of-the-link-mass" class="headerlink" title="Effect of the link mass"></a>Effect of the link mass</h6><p>连杆对电机有两个显著影响,它增加了额外的惯性,并由于臂的重力增加了转矩,两者随关节位型而变化.以下图为例,考虑2-平面机械臂.第一连杆为红色,假设质量集中在质心处,额外的惯量(extra inertia)记为$m_1r_1^2$.电机还将受蓝色杆的影响,惯量取决于$q_2$–直臂的惯性大于折叠情况.</p><p><img src="3.png" alt=""></p><p>我们将看到作用在红色连杆质心处重力将在关节1电机上产生与$\cos{q_1}$正比的力矩.而在蓝色杆上的杠杠效应则更大.</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; mdl_twolink_sym</span><br><span class="line">&gt;&gt; syms q1 q2 q1d q2d q1dd q2dd <span class="built_in">real</span></span><br><span class="line">&gt;&gt; tau = twolink.rne([q1 q2], [q1d q2d], [q1dd q2dd]);</span><br></pre></td></tr></table></figure><p>$\tau_1 = M_{11}\ddot{q}+ M_{12}(q_2)\ddot{q_2} + C_1(q_2)\dot{q_1}\dot{q_2}+ C_2(q_2)\dot{q_2}^2+g(q_1,q_2)$,    (1)</p><p>$g = (a_1m_1+a_1m_2+c_1m_1)\cos(q_1)+ (a_2m_2+c_2m_2)\cos(q_1+q_2)$</p><p>我们可以通过课本提供的工具箱确定作用在每个关节的力矩(作为关节位置,速度,加速度的函数).</p><h6 id="Gearbox"><a href="#Gearbox" class="headerlink" title="Gearbox"></a>Gearbox</h6><p>电动机产生不了非常大扭矩.其输出转速可以非常高,因此配合齿轮减速装置来降低转速提高转矩.齿轮变速箱缺点是增加了成本,重量和摩擦噪声.许多高性能机器人会采用昂贵大扭矩电机直接驱动,或使用薄金属带而不是齿轮来获得一个非常低的传动比.</p><p><img src="4.png" alt=""></p><p>连杆惯性矩阵记为$J_l$.运动引起的力矩记为$\tau_d$.如上图.在参考系$l$测量值与$m$系下的参考值有关.</p><p>$^l\tau_C=G\cdot ^m\tau_C, ^l\omega = ^m\omega/G$.</p><p>从电机看,其惯性由2部分组成,其一是电机本身的旋转部分,即转子(rotor),记为$J_m$.该值在制造商数据手册中会提供.其二是可变的负载惯性$J_l$(variable load inertia),是动连杆(driven link)及连接到它的所有连杆惯性之和.对于关节$j$,它就是方程(1)构造的惯性矩阵元素$M_{jj}$.</p><h6 id="Modeling-the-robot-joint"><a href="#Modeling-the-robot-joint" class="headerlink" title="Modeling the  robot joint"></a>Modeling the  robot joint</h6><p>完整的电机驱动器包括产生转矩的电机,齿轮箱(gearbox,变速箱,放大转矩并减小负载的影响),编码器(位置,速度反馈).如下图</p><p><img src="5.png" alt=""></p><p>建立电机轴(motor shaft)力矩平衡方程(在参考系$m$下):</p><p>$K_mK_au- B’\omega -\tau’_C -\frac{\tau_d(q)}{G} = J’\dot{\omega}$,(2)</p><p>其中$B’,\tau’_C,J’$,</p><p>分别由电机,齿轮箱,和负载引起的有效总粘性摩擦,库伦摩擦和惯性.</p><p>$B’=B_m+ \frac{B_l}{G^2},J’=J_m+ \frac{J_l}{G^2}$.</p><p>为了分析(2),首先将其线性化: $B’\omega+J’\dot{\omega} = K_mK_au$.</p><p>再经Laplace变换: $sJ’ \Omega(s) + B’\Omega(s) =K_mK_aU(s)$. 其中$\Omega(s),U(s)$分别是时域信号$\omega(t),u(t)$的laplace变换.</p><p>将其改写为<fonn color=red>电机速度到控制输入&lt;\font&gt;的线性传递函数:$\frac{\Omega(s)}{U(s)} =  \frac{K_mK_a}{J’s+B’}$.</p><p>以puma 560第二个关节为例.在其它参数未给明情况下,取$B’ =B_m$,由$J’ =J_m+ \frac{J_l}{G^2}$可计算其有效惯量($M_{22}$随位型而变化,假设其为 $4.43kg/m^2$,则计算$J’ =5.8\times 10^{-4} kg/m^2$)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mdl_puma560</span><br><span class="line">syms qn</span><br><span class="line">tf &#x3D; p560.jointdynamics(qn);</span><br><span class="line">tf(2)</span><br><span class="line">&gt;&gt;</span><br><span class="line">ans &#x3D;</span><br><span class="line">            1</span><br><span class="line">  ----------------------</span><br><span class="line">  0.0005797 s + 0.000817</span><br><span class="line">Continuous-time transfer function.</span><br></pre></td></tr></table></figure><p> 一旦有了这种形式的模型,我们可以绘制阶跃响应(step response)并使用一系列标准控制系统设计工具.</p><h6 id="Velocity-control-loop"><a href="#Velocity-control-loop" class="headerlink" title="Velocity control loop"></a>Velocity control loop</h6><p> <font color=red>嵌套</font>的控制回路是一种常见的控制结构. 其outer loop(外环)负责保持关节位置(p),并确定使位置误差最小的关节速度(velocity of joint);inner loop(内环,速度环)负责保持外环所需的关节速度.电机速度控制对所有类型机器人都很重要,不仅仅是串联机械臂.例如第4章中所讨论的小车车轮速度和四旋翼转子速度.</p><p>simulink模型如下.电机驱动器输入为所需速度和实际速度误差.1ms延迟用来模拟速度控制算法的计算时间,而饱和器(saturator)模拟电机提供的最大力矩.</p><p><img src="6.png" alt=""></p><p><img src="7.png" alt=""></p><p>考虑比例控制情况$K_i=0$, $u^* =K_v(\dot{q}^*-\dot{q})$.其中输入$\tau_d$用于模拟作用在关节上的干扰力矩.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vloop_test</span><br></pre></td></tr></table></figure><p>经过少量实验, 当$K_v=0.6$可获得满意的性能.在不连续处会有轻微过冲(overshoot),但增益越小,会导致速度误差越大,而增益对大,会导致振动(oscillation),控制工程需要权衡.</p><p>从实验结果(运行上述代码)来看,可观察到非常小的稳态误差(steady-state Error)-实际速度始终低于要求.从经典控制系统角度来看,速度环不包括积分模块(integrator block),并且归类为0类系统(type 0 system,对于恒定输入表现出有限的误差).更直观说,为了以恒定速度移动,电机必须产生有限的力矩来克服摩擦,由于电机力矩与速度误差成正比,因此存在有限速度误差.</p><p>接下来我们研究惯性变化对闭环控制的影响.结果显示对于8连杆惯性,不稳定,对于较大的惯性,跟踪误差衰减.</p><h6 id="Position-control-loop"><a href="#Position-control-loop" class="headerlink" title="Position control loop"></a>Position control loop</h6><h6 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h6><h5 id="Rigid-body-dynamics-compensation"><a href="#Rigid-body-dynamics-compensation" class="headerlink" title="Rigid body dynamics compensation"></a>Rigid body dynamics compensation</h5><h6 id="Feedforward-control"><a href="#Feedforward-control" class="headerlink" title="Feedforward control"></a>Feedforward control</h6><h6 id="Computed-torque-control"><a href="#Computed-torque-control" class="headerlink" title="Computed torque control"></a>Computed torque control</h6><h6 id="Operational-space-control"><a href="#Operational-space-control" class="headerlink" title="Operational space control"></a>Operational space control</h6><h5 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h5><h5 id="Summary-1"><a href="#Summary-1" class="headerlink" title="Summary"></a>Summary</h5>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;讨论串联机械臂的动力学与控制,在simulink上测试机器人三环控制,参考书为robotics,vision,and contr
      
    
    </summary>
    
    
      <category term="动力学" scheme="https://5imcs.com/categories/%E5%8A%A8%E5%8A%9B%E5%AD%A6/"/>
    
    
      <category term="仿真" scheme="https://5imcs.com/tags/%E4%BB%BF%E7%9C%9F/"/>
    
      <category term="simulink" scheme="https://5imcs.com/tags/simulink/"/>
    
  </entry>
  
  <entry>
    <title>读论文continuous control with deep reinforcement learning并测试walking robot学习实例</title>
    <link href="https://5imcs.com/posts/1080553f/"/>
    <id>https://5imcs.com/posts/1080553f/</id>
    <published>2020-07-05T16:00:00.000Z</published>
    <updated>2020-07-24T09:22:01.003Z</updated>
    
    <content type="html"><![CDATA[<h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><ul><li>许多物理任务具有连续且高维的动作空间. google Deepmind(DDPG.论文算法)使用actor-critic结构,用于连续动作的预测.</li><li>以walking robot为例,训练其走直线并保证最省能量控制.</li></ul><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul><li>RL(见补充:1)</li></ul><p>可解决一些非常困难的控制问题(无人机,机器人).与其它使用静态数据集学习框架(无监督\有监督)不同,RL与动态环境交互,旨在找到”收集奖励最多”的动作序列.</p><p>假设agent里有一个大脑,它接收状态观测observations,输出动作.这种映射称为policy.如果提供一组观测量,策略便会确定要采取的动作.如果将策略表示为神经网络,则可以让agent一次接收上千输入状态,并给出有意义的动作.当环境不断变化,这种”策略”不再最优或无法映射到有效动作,强化学习算法RL—(根据采取的行动,从环境中观察的结果以及所获得的奖励reward,<br>,update)–&gt;policy. 从而使得给定任何状态,agent都可以采取最有利的动作以产生最多的奖励累计.比如这次没考好,贪玩了,则调整策略,下次好好学习.</p><p>强化学习核心是优化问题(让计算机自主学习优化参数,”反复试错”).<br>基本关键词有: policy,state,action,reward(特定状态下的instantaneous奖励), value(从该状态到未来的全部奖励)等…所以一般评估状态的价值,而不是评估奖励(见Q-learning中的Q表更新方式),这样可以帮助agent选择一段时间内可获得的最优奖励的动作(而非短期利益).</p><p>基本分类:policy function based(如policy gradients方法), value function based(Q-learning), actor-critic based(DDPG,论文采用).</p><center><pre class="mermaid">graph LR    A[Policy <br> RL]-->|Actions| B[states <br> reward]    B-->|Reward| A    B-->|State Observations| A</pre></center><p>关于model-free(agent doesn’t need to know anything about the environment at all)和model-based(we know what’s not worth exploring)</p><p>优缺点: 大量样本,大量试错….如何理解神经网络的权重和偏差值,如果策略不符合规划,如何调整这个策略.把所有函数压缩在一个黑盒子,一旦出现问题,很难手动定位到出问题的点,无法解释输出为何这样.</p><ul><li><font color=red>Q学习</font> (Q表记录潜在奖励,以学生写作业为例,a1–看电视, a2–写作业)</li></ul><table><thead><tr><th></th><th>a1</th><th>a2</th></tr></thead><tbody><tr><td>s1</td><td>-2</td><td>1</td></tr><tr><td>s2</td><td>-4</td><td>2</td></tr></tbody></table><p>关键: 如何更新?</p><p>根据Q表的估计,在s1状态采取了a2(写作业),并到达了a2状态,这时开始更新决策Q表: 想象在s2中采取行为,分别看哪种行为对应Q值大.</p><p>比如说Q(s2,a2)比Q(s2,a1)大(预估的).我们将$\gamma* maxQ(s2,a’) + r$ (最大的Q乘以衰减值 + 达到s2所获奖励),因为这里获得实际的奖励,于是将其作为现实Q(s1,a2)的值.之前的Q(s1,a2)为估计值.</p><p>注: Bellman方程的表现形式，它表明了当前状态s下Agent所能够获得的期望回报值与其后继状态之间存在关联.</p><p>有了现实与估计值,就能更新Q(s1,a2):</p><p>$ Q(s,a)=Q(s,a)+\alpha*[r+\gamma*maxQ(s’,a’)-Q(s,a)] $($\alpha$学习效率).</p><p>算法流程:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">initial Q(s,a)</span><br><span class="line">repeat(for each episode):</span><br><span class="line">initial s</span><br><span class="line">repeat(for each step of episode):</span><br><span class="line">choose a from s using policy derived from Q (eg: epsilon greedy)</span><br><span class="line">take a, observe r,and s&#39;</span><br><span class="line">$ Q(s,a)&#x3D;Q(s,a)+\alpha*[r+\gamma*maxQ(s&#39;,a&#39;)-Q(s,a)] $</span><br><span class="line">s &#x3D; s&#39;</span><br><span class="line">until s is terminal</span><br></pre></td></tr></table></figure><p>一个例子: —O—–| (O代表人,|表示宝藏,训练它快速发现宝藏) 视频链接:<a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-1-general-rl/" target="_blank" rel="noopener">^1</a></p><ul><li><font color=red>DQN(Deep Q network,2013,deepmind)</font>, 针对具有高维observation spaces以及离散且低维action sapces的agent训练,比如: 训练计算机玩电子游戏(离散的按键动作). </li></ul><p>关键: 融合了神经网络和Q-学习</p><p>Q学习用表格存储状态及其行动所拥有的Q值.当状态非常多时,如下围棋,计算机内存不够.</p><p>解决方法: </p><ol><li>利用神经网络,将状态s和动作a作为神经网络的输入,经分析得到(s,a)的Q值.这样一来就没必要继续在表格中记录Q值. 神经网络相当于眼鼻口接受外部信息,经大脑加工输出每种动作a的值,通过强化学习的方式选择动作.</li><li>更新神经网络, 训练样本怎么来? 采取的做法是用Q-学习中的Q现实值表示正确的Q值,神经网络的参数通过旧神经网络的参数+学习率*(Q现实-Q估计)来更新.</li><li>通过神经网络预测Q(s’,a’)值.选取Q最大相应的动作来获取环境的reward.</li><li>Experience replay:将系统探索环境数据储存起来.然后随机采样样本更新神经网络的参数.</li></ol><ul><li><font color=red>policy gradients</font> 论文链接:<a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf" target="_blank" rel="noopener">^2</a></li></ul><p>上述两种方法主要是根据奖惩值,学习正确行为.如果是在无穷多(连续)动作中计算价值,吃不消.</p><p>思路: 通过神经网络分析,选取行为,再利用奖惩(不求导情况下),在神经网络反向传播中,左右下次被选的可能性.</p><p>例子: CartPole(传统强化学习任务之一:控制倒立摆): <a href="https://gym.openai.com/envs/CartPole-v0/" target="_blank" rel="noopener">^3</a> (注:其实DQN方法也能解决,将任务简化为cart只有离散动作R,L,状态为杆的位置及速度,但如果对倒立摆施加连续的扭矩这种方法就不合适了)</p><p>reward = $(-\theta^2-\dot{\theta})^2 $</p><h4 id="论文思路"><a href="#论文思路" class="headerlink" title="论文思路"></a>论文思路</h4><ul><li>DDPG(deep determinstic policy gradient, actor-critic)</li></ul><p>在控制中我们需要连续的动作空间,就像对倒立摆问题施加连续的力矩.将policy function based,value function based两种技术合并到一类称为执行-评价器(actor-critic)的算法中,其中执行器<br>是一个network,试图采取它认为当下状态最好的动作,评价器network则试图估计状态的价值和执行器采取的行动. </p><center><pre class="mermaid">graph LR    C[Observations]-->A[Actor]    C-->B[critic]    B-->|Value|D[Compare]    A -->|Actions|E[Environment]    E-->|Reward|D</pre></center>适用于连续动作空间,因为critic只需要查看一个动作,而不是试图通过评估所有动作.基本原理为,Actor采取与策略函数相同的方法(见policy gradient)选择动作.评价器则预估它认为状态-动作的值,然后使用环境反馈的奖励来确定它预测的价值准确程度,之后再类似神经网络的反向传播进行自我更新.Actor调整将来再次采取该动作的概率.(该策略会沿着评价器建议的方向进行梯度上升,执行器从评价器反馈中学习正确的动作,从而了解动作好坏.评价器从所获得奖励中学习价值函数,以便正确评价执行器采取的动作).agent利用策略和价值函数算法的最佳部分.A-C能处理连续状态和动作空间.<p>最后可将优化的静态策略部署到物理环境中,也可以选择将RL算法一并部署以继续学习.</p><h4 id="walking-robot例子-model-based-可从Matlab文件中心下载该实例-视频讲解及源码-4"><a href="#walking-robot例子-model-based-可从Matlab文件中心下载该实例-视频讲解及源码-4" class="headerlink" title="walking robot例子 (model-based) 可从Matlab文件中心下载该实例. 视频讲解及源码^4"></a>walking robot例子 (model-based) 可从Matlab文件中心下载该实例. 视频讲解及源码<a href="https://www.mathworks.com/videos/deep-reinforcement-learning-for-walking-robots--1551449152203.html" target="_blank" rel="noopener">^4</a></h4><p>从传统控制角度:</p><p>observations(eg: camera images) -&gt; feature extraction -&gt; state estimation( other sensors) -&gt; control(blance, leg.motor control…) -&gt; 行走 </p><p>从RL(end-to-end)角度:</p><p>observations -&gt; (Black Box) -&gt; motor command</p><ul><li>分析:</li></ul><ol><li><p>actions(torque)<br>left-right ankle\knee\hip, 机器人躯干\腿和其所处世界构成环境.</p></li><li><p>observations<br>来自环境的观测值,基于传感器的类型和位置.本例中有31个观测值,具体包括躯干的Y,Z轴坐标值,沿X,Y,Z方向躯干速度,躯干旋转角度和角速度,以及6个关节的角度\角速度,脚与底面接触力.以及反馈上个时间步输出的力矩指令.</p></li></ol><p>控制系统(RL agent)接受31观测值,并且连续不断计算6个力矩.它使用一个actor将31观测值映射到6个力矩动作,以及一个评价器让执行器更有效率.</p><ul><li>工作流程</li></ul><ol><li>设置具有足够参数的策略.</li></ol><p>A-C神经网络.</p><ol start="2"><li>选择reward来奖励良好的控制.</li></ol><p>reward = $v_x+ 0.00625-50*z^2 - \sum(\tau_i^2) + -3y^2$</p><ol start="3"><li>选择RL算法观察(s,a),以及调整参数(traning过程).</li></ol><p>DDPG</p><h4 id="继续研究的方向"><a href="#继续研究的方向" class="headerlink" title="继续研究的方向"></a>继续研究的方向</h4><ol><li>最优控制结果作为样本训练神经网络?</li><li>用神经网络来训练轨迹规划参数?(已有不少文献)</li><li>流形学习? (流形学习估计柔性臂位姿)</li></ol><h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><ol><li><a href="https://ww2.mathworks.cn/videos/series/reinforcement-learning.html" target="_blank" rel="noopener">https://ww2.mathworks.cn/videos/series/reinforcement-learning.html</a> <a href="https://www.bilibili.com/video/BV1Gg4y1v7TR?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Gg4y1v7TR?p=2</a></li><li><a href="https://www.davidsilver.uk/teaching/" target="_blank" rel="noopener">https://www.davidsilver.uk/teaching/</a> </li><li>3BlueBrown(神经网络)<br><a href="https://www.bilibili.com/video/BV1bx411M7Zx?from=search&amp;seid=3993471132113230062" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1bx411M7Zx?from=search&amp;seid=3993471132113230062</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;许多物理任务具有连续且高维的动作空间. google Deepmind(DDPG.论文算法)使用actor-critic结构,用
      
    
    </summary>
    
    
      <category term="Deep Reinforcement Learning" scheme="https://5imcs.com/categories/Deep-Reinforcement-Learning/"/>
    
    
      <category term="笔记" scheme="https://5imcs.com/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>effective sampling in SE(3)</title>
    <link href="https://5imcs.com/posts/e9246c12/"/>
    <id>https://5imcs.com/posts/e9246c12/</id>
    <published>2020-04-23T07:21:05.013Z</published>
    <updated>2020-04-23T07:21:05.037Z</updated>
    
    <content type="html"><![CDATA[<h5 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h5><ul><li>在$\rm SE(3)$定义合适的距离,采样函数,以及采样点插值.</li></ul><p>参考[1] </p><h5 id="大致思路"><a href="#大致思路" class="headerlink" title="大致思路"></a>大致思路</h5><ol><li>对$\rm SO(3)$均匀采样(欧拉角,四元数)</li></ol><p><img src="sampling_sphere.png" alt=""></p><ol start="2"><li>Distance metics on $\rm SE(3)$ and geodesic interpolation function for rotations.</li></ol><p><img src="checkinterpolation.png" alt=""></p><h5 id="一个数值小实验"><a href="#一个数值小实验" class="headerlink" title="一个数值小实验"></a>一个数值小实验</h5><p>考虑22自由度蛇形机械臂的路径规划问题,采用RRT算法:</p><p><img src="result_withobs_rrt.png" alt=""></p><p>注:</p><ul><li><b>$\rm SE(3)$上的快速搜索随机生成树</b></li></ul><p>1.记初始位姿 $Q_0\in \rm SE(3)$  为随机树 $T$ 根结点.</p><p>2.预先给定某阈值 $p \in (0,1]$  . 随机生成某数 $p_c \in (0,1)$ , 若 $p_c\leq  p$, 则在工作空间中随机采样, 采样点记为 $Q_{rand}$ ; 否则直接取定目标位姿为采样点 $Q_{rand}$ .树$T$中距离${Q_{rand}}$ 最近的结点记为 $Q_{father}$, 记由 $Q_{father}.{x}$指向 ${Q_{rand}}.{x}$方向上步长为$p_{step}$处的结点为$Q_{new}.{x}$.</p><p><font color = red>$Q_{new}.R$可利用四元数与旋转矩阵的关系插值给出, 其中$Q_{<em>}.{x}$和$Q_{</em>}.{R}$分别表示采样点$Q_{*}$位置和姿态.</font> </p><p>3.若机械臂位姿$Q_{new}$不与障碍物发生碰撞, 则将$Q_{new}$插入树$T$. </p><p>重复采样直至$T$到达目标位姿, 从而获得一条由${Q_0}$到目标位姿${Q_d}$的规划路径.</p><ul><li><b>如上图,数值小实验可优化的空间在于</b></li></ul><ol><li>do smoothing before using the path.从上图可以看到,RRT的路径不光滑,也存在走”弯路”的情况.所以在应用时,应该考虑对路径做一些处理,如shortcutting,其思路是:随机连接生成树路径的采样点,如果该’线段’可行,则更新到路径中”.</li></ol><p>具体的,参见 see: <a href="http://www.osrobotics.org/osr/planning/post_processing.html" target="_blank" rel="noopener">http://www.osrobotics.org/osr/planning/post_processing.html</a> </p><p>[1]: Effective sampling and distance metrics for rigid body path planning</p><!--![](file.pdf)--><!--<div class="pdf" target="file.pdf" height=""></div> -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;在$\rm SE(3)$定义合适的距离,采样函数,以及采样点插值.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考[1] &lt;/p&gt;
&lt;h5 i
      
    
    </summary>
    
    
      <category term="sampling" scheme="https://5imcs.com/categories/sampling/"/>
    
    
      <category term="Matlab" scheme="https://5imcs.com/tags/Matlab/"/>
    
  </entry>
  
  <entry>
    <title>virtualbox虚拟硬盘扩容与诊所网页项目测试</title>
    <link href="https://5imcs.com/posts/8090a50b/"/>
    <id>https://5imcs.com/posts/8090a50b/</id>
    <published>2020-03-24T08:06:09.859Z</published>
    <updated>2020-03-24T08:06:09.867Z</updated>
    
    <content type="html"><![CDATA[<h5 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h5><ol><li><p>原本该虚拟硬盘是测试opencv的,所以划分了10G(固定大小),昨天拿来测试了下一个angular,nodejs的网页项目,发现硬盘容量不够了,这次增加到20G.</p></li><li><p>记录一下nodejs,angualr以及mongo基本命令,了解下如何部署前后端分离的网站.</p></li></ol><h6 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h6><p>步骤:</p><p>1.添加新的虚拟硬盘ubuntuS.vdi,并插入Gparted光盘.</p><p><img src="b34e207f.png" alt=""></p><p><a href="https://gparted.org/download.php" target="_blank" rel="noopener">GParted – Download</a></p><p><a href="https://sourceforge.net/projects/gparted/files/gparted-live-stable/1.1.0-1/gparted-live-1.1.0-1-amd64.iso/download?use_mirror=ayera" target="_blank" rel="noopener">Download GParted from SourceForge.net</a></p><ol start="2"><li>启动,并且</li></ol><blockquote><p>sudo dd if=/dev/sda of=/dev/sdb</p></blockquote><p>使用Gparted查看,并核对下,从sda对拷到sdb.</p><p>结束后,将一些未配置的空间(右键ext4)配置到sdb.</p><ol start="3"><li>最后将旧的虚拟硬盘及光盘移除.</li></ol><h6 id="网页项目本地部署"><a href="#网页项目本地部署" class="headerlink" title="网页项目本地部署"></a>网页项目本地部署</h6><p>待续</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h5&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;原本该虚拟硬盘是测试opencv的,所以划分了10G(固定大小),昨天拿来测试了下一个angular,nodejs的网页项
      
    
    </summary>
    
    
      <category term="Notes" scheme="https://5imcs.com/categories/Notes/"/>
    
    
      <category term="virtualbox" scheme="https://5imcs.com/tags/virtualbox/"/>
    
  </entry>
  
  <entry>
    <title>读论文path planning and collision avoidance for robots并仿真</title>
    <link href="https://5imcs.com/posts/c7ca1a9b/"/>
    <id>https://5imcs.com/posts/c7ca1a9b/</id>
    <published>2020-03-18T09:57:18.453Z</published>
    <updated>2020-04-04T02:23:50.270Z</updated>
    
    <content type="html"><![CDATA[<h5 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h5><ul><li>通读论文并实现论文的例子</li></ul><p><img src="scene.png" alt=""></p><ul><li>搭建一个基本的框架(包含问题描述,障碍物避障准则(代数层面),离散,B-spline,SQP等部分),将其仿真实现,并可应用于n-dof的机械臂.</li></ul><p>仿真如下:</p><p>论文$t_f$最优时间为0.4261s, Matlab返回(局部?)最优解为0.4412</p><p><img src="testAnimated.gif" alt=""></p><p><font color=red>这篇文章对于最优控制的框架[^1]值得学习,尤其利用代数将避障处理成不等式约束,再利用隐面剔除策略减少约束.本文在前面newton-euler dynamics算法的基础上,结合B样条将问题离散,简单(在未引入隐面剔除策略下,调用<code>fmincon</code>函数)实现论文的optimal time数值算例.</font></p><h5 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述:"></a>问题描述:</h5><ol><li>动力学方程</li></ol><p>可参考上一篇博文,这部分算法已经在Vrep中验证过了.简单验证一下是否和论文的一样 p462 </p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% grav = inversedyn(robot.thetalist, zeros(3, 1), zeros(3, 1) ,robot.g, robot.T, robot.InertiaMtr, robot.Twist)</span></span><br><span class="line"><span class="comment">% k = -9.81;</span></span><br><span class="line"><span class="comment">% F_q_u = [0; -k*l*cos(robot.thetalist(2))*(0.5*m2+m3+m4)-k*l*cos(robot.thetalist(2)+robot.thetalist(3))*(0.5*m3+m4);...</span></span><br><span class="line"><span class="comment">%             -k*l*cos(robot.thetalist(2)+robot.thetalist(3))*(0.5*m3+m4)]</span></span><br></pre></td></tr></table></figure><blockquote><p> 核对无误 但是输入文章fig9的u1,u2,u3输入进去,杆1不怎么移动,这里应该是论文fig9有问题,我们将其乘以scaling factor 100. aha,作者回复我了,说忘记在论文提fig9是经过normalized,实际边界在-100~100Nm.此外,作者建议不要考虑重力(insufficient control 将导致robot崩溃 - -),关于初值的选取作者建议除了论文中的scaling方法,还可以从无障碍物开始训练,以constructing good initial guess.本博文也将采用这一策略.</p></blockquote><ol start="3"><li>避障准则</li><li>B-spline 直接离散化</li><li>SQP</li><li>隐面剔除策略</li></ol><h5 id="序列二次规划-SQP"><a href="#序列二次规划-SQP" class="headerlink" title="序列二次规划(SQP)"></a>序列二次规划(SQP)</h5><p>因为时间有限,本此仿真将利用Matlab优化函数处理离散最优问题.先对<code>fmincon</code>有个大概了解.后续腾出手来再补充SQP.</p><p>作为Nonlinear programming solver,求解如下形式:<br>$$ min_x f(x) ~ s.t : c(x)\leq 0, ceq(x) = 0, A\cdot x \leq b, Aeq\cdot x = beq, and, lb \leq x \leq ub  $$</p><h5 id="开发进度"><a href="#开发进度" class="headerlink" title="开发进度"></a>开发进度</h5><ul><li><p>spline_curve(torque_grid_point, t, order, knot_vector),样条离散,输入 $t \in [0,1]$ 计算torque. 关于初值的选取我问了论文作者,他说如果没记错的话,对于torque control取了21个控制点(de boor point), 一阶样条,与auxiliary control $\omega$一致.</p></li><li><p>边界约束</p></li></ul><p>$$u(t)\in U: = {u\in \mathbb{R}^m | u_{min} \leq u \leq u_{max} }$$</p><p>$$ \omega_{i,k} \geq 0, for~ i = 1,…,12, k = 1,…,21 $$ </p><p>(为和论文保持统一,只考虑load和obstacle,每个cuboid六个面, 共12次, 时间节点划分为21个.)</p><ul><li>关于线性等式约束和线性不等式约束.</li></ul><p>无. </p><ul><li>关于非线性等式约束和非线性不等式约束.</li></ul><p>粗略一看,边界约束属于线性等式约束,在这个离散最优问题上,我们对动力学方程离散有两种方式: 1.由各时间节点 joint angle进行样条离散,2.是按论文的,对control进行离散.</p><p>从动力学方程的角度,似乎不太好按第一种方式进行离散,因为时间最优控制一般是bang-bang控制,关节轨迹一般需要比较高阶的样条(但在initial_guess时会更直观一点吧?).不过结果应该差不多吧,按这种方式离散, 上述边界约束则是非线性约束.</p><p>我还是按论文走.对control离散之后,调用正动力学计算$\ddot{\theta}$,选择合适的积分策略(这里简单选择euler显示格式,因为步长划分比较多).再调用正运动学计算负载位姿$T_{load} = [R,p;0 0 0 1] $,得到离散后避障约束方程(P444,OCP Problem)以及关节边界约束:</p><p>$$   G_I(x_k(z))^T w_I,h(t_k) =  0, I = 1,…,M=12,~~ K = 0,…,N $$<br>$$   g_I(x_k(z))^T w_I,h(t_k) &lt;= -e $$<br>$$   \phi(x_o,x_N(z)) = 0; $$</p><p>在程序中我设置了两个模式,以便切换.</p><blockquote><p><code>robot.collision_mode = 0; %false = 0 默认不考虑</code>, 进行最省时间路径规划</p></blockquote><blockquote><p>Q1.错误使用 barrier<br>Nonlinear constraint function is undefined at initial point. Fmincon cannot continue.</p></blockquote><p>遇到这个问题需要检查一下,调用<code>nonlinear_constraint(initial_guess)</code> 看是不是有NaN.(问题主要出在计算正动力学时 $ M $ 矩阵奇异.这与我们初值选取有关,如果想要避免这一问题,初始值可以稍微小一点.)</p><blockquote><p>Q2.Local minimum found that satisfies the constraints. 弄了个乌龙,返回满足局部最优,于是将其绘制轨迹曲线,发现都为0, 才发现最优化问题改变了x(1:6,1)这几个值,边界条件得管好呀..</p></blockquote><!--未经过任何优化,运行1min左右,得到了一个(忽略障碍)局部(非bang-bang,应该还有继续优化空间)最优时间解.<font color=red>如下</font>![](release_1_without_obstacles.png) --><p> 有了这么个解之后,可以做很多事情了.</p><blockquote><p>关于为什么将$[0,tf]$转化到$[0,1]$</p></blockquote><p>如果我没理解错的话,除了为了分析Hessian之外,在编程中我发现,如果tf过大,划分步长不够的话,<code>forwardynamics</code>函数容易积累误差.想必这也是论文作者敢划分21点的原因,平均下来步长也就0.05,perfect!</p><ul><li>局部最优与全局<blockquote><p>探讨一下全局的策略.待完善. 涉及SQP一些技术手段,待后面再讨论. 本文仅在运行程序时做一些小记录.</p></blockquote></li></ul><p>在训练initial guess的时候, 训练到了<code>iter = 10</code>,从下午六点到晚上八点半才训练出一个较好的初值.这次优化包括了<code>6+ 3*21 + 12*21 + 1</code>个变量,其实应该像论文中所采取的做法,将Hessian输入到sqp求解中,能有效提高计算效率.经过我的测试,一个good initial guess可以将计算缩短,我在目标函数中加入了惩罚项 $\alpha||norm(\omega_i)||$,可以稍微提高鲁棒性.</p><p><font color=red>对大于20维优化问题,不建议用Matlab <code>fmincon</code>,本笔记只做一个简单的测试.具体实践中,还应该吃透一些常用的优化算法,编写好优化问题的梯度与Hessian.</font></p><ul><li>exitflag</li></ul><p>-2: No feasible point<br>-1: The output function terminated the algorithm<br> 0: Iterations exceeded options.<br> 1: 某种情况达到了收敛. 具体见doc fmincon.</p><ul><li>数值结果</li></ul><p>与论文控制力有些差别,计算差异体现在积分策略,以及 $\omega$ 的边界条件上.</p><p>注: 经测试,选择相同fmincon的优化算法,matlab 2019b与2015b均可以跑到相同结果.</p><p><img src="Res.png" alt=""><br><img src="omega.png" alt=""><br><img src="res.png" alt=""></p><ul><li>关于梯度与Hessian阵.</li></ul><p>这是一个可以对论文改进的方向.博文到这里, 耗费一周时间总算完成了一个OCP问题小实验.</p><hr><h5 id="程序优化"><a href="#程序优化" class="headerlink" title="程序优化"></a>程序优化</h5><ul><li><input disabled="" type="checkbox"> 因为论文采用的是一阶样条, 为了精简计算, 可以使用表达式避免调用</li></ul><p>$$ u = u_i + \frac{(t- t_i)}{t_{i+1}-t_i}(u_{i+1}-u_i) , t \in [t_i-t_{i+1}] $$</p><ul><li><input disabled="" type="checkbox"> 编写程序贪快,可以将一些参数变量放到<code>main</code>函数.</li><li><input disabled="" type="checkbox"> 隐面剔除</li><li><input disabled="" type="checkbox"> Hessian矩阵</li><li><input disabled="" type="checkbox"> <a href="https://ww2.mathworks.cn/products/parallel-computing.html" target="_blank" rel="noopener">Parallel Computing Toolbox - MATLAB</a> 并行计算</li></ul><hr><p>参考文献</p><p>[^1]: path planning and collision avoidance for robots.</p><hr><p>补充</p><p>/release1.1_with_obstacles/ ReswithObs.mat 为 tf = 0.4412, exitflag = 2;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%% ---引用</span><br><span class="line">load ReswithObs.mat</span><br><span class="line">initial_guess &#x3D; Res;</span><br></pre></td></tr></table></figure><p>./mainprogram_spline_dev.m 为 tf = 0.4248, exitflag = 1; </p><p><code>fmincon</code>在高维上做的并不是非常出色,用同伦的方法找初值是一个办法,这也是一个不断试错的过程.<br>但是如果可能,还应该对程序优化,如梯度与Hessian,剔除策略.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;通读论文并实现论文的例子&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;scene.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
      
    
    </summary>
    
    
      <category term="optimal control" scheme="https://5imcs.com/categories/optimal-control/"/>
    
    
      <category term="Matlab" scheme="https://5imcs.com/tags/Matlab/"/>
    
      <category term="仿真" scheme="https://5imcs.com/tags/%E4%BB%BF%E7%9C%9F/"/>
    
  </entry>
  
  <entry>
    <title>coordinate-invariant dynamics algorithm验证</title>
    <link href="https://5imcs.com/posts/1e752288/"/>
    <id>https://5imcs.com/posts/1e752288/</id>
    <published>2020-02-29T16:00:00.000Z</published>
    <updated>2020-07-12T12:52:48.391Z</updated>
    
    <content type="html"><![CDATA[<h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><ul><li>以平面RRR三连杆和空间三连杆为例,通过选择两种标架(joint frame以及center of mass frame),实现坐标不变算法对机械臂正逆动力学仿真:包括对inertial matrix, energy的分析; </li><li>搭建一个基本的creatrobot函数(其中各link为cuboid),采用euler数值积分,将仿真可视化,并与商业软件V-rep对比(对比末端轨迹),在此基础上测试变分积分子.</li></ul><h4 id="Coordinate-invariant-algorithm"><a href="#Coordinate-invariant-algorithm" class="headerlink" title="Coordinate-invariant algorithm"></a>Coordinate-invariant algorithm</h4><p>criag的书采用的就是这一方法,李泽湘的新的lecture也加入了这部分内容,坐标系建立在关节处,这一点与Park不同.</p><!--起初研究这部分内容,我还产生了一个疑问,为了保证惯性矩阵Inertia Matrix的简洁优雅,所以计算是在center of mass这一标架上进行的,$\tau = F^T_iS_i$, 其中$S_i$为screw aixs of i joint expressed in {i} frame.那么$\tau$结果应该是施加在第i杆质心上的?[^_^]: <https://robotics.stackexchange.com/questions/20249/newton-euler-inverse-dynamics-by-screw>先说结果,从程序实验来看,其实两种方法实验结果一致(包括得到的力矩).我还没有仔细证明,但思路应该是要利用质心Inertia Matrix是$I_b$,那么在关节处(frame {s})应为为$Ad^T_gI_b Ad_g$. --><!--<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">Is</span> = <span class="title">inertialmatirx_check</span><span class="params">(T,Ib)</span></span></span><br><span class="line"><span class="comment">%测试 I_s = Ad^T_T(bs)I_bAd_T&#123;bs&#125; 将cm惯性矩阵转换到joint处</span></span><br><span class="line"><span class="comment">%&#123;</span></span><br><span class="line"><span class="comment">Mb = diag([172.27656, 172.27656, 172.27656, 3.16441, 32.90891, 34.31683]);</span></span><br><span class="line"><span class="comment">Rbs = [0.98655, 0.16044, 0.03136; 0.0005, 0.18889, -0.982; -0.16347, 0.9688, 0.18627];</span></span><br><span class="line"><span class="comment">Pbs = [0.74197; 0.019914; -0.013597];%Pbs = [0;0;0];%</span></span><br><span class="line"><span class="comment">Tbs = [Rbs, Pbs; 0 0 0 1];</span></span><br><span class="line"><span class="comment">Is = inertialmatirx_check(Tbs,Mb);</span></span><br><span class="line"><span class="comment">%&#125;</span></span><br><span class="line">Is =  Adjoint(T)'*Ib*Adjoint(T);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>–&gt;</p><!-- <s>这里得到的Inertial Matrix不是[I_c ,0 ; 0 ,m]形的,目前我没找到相关的文献描述,但从推导来看,是没有问题的(加上惯性矩阵的转换,应该可以推导出下面两种方式等价).如果将对角置0,下述程序得不到相同的结果.</s> 纠正: 广义惯性矩阵 --><h5 id="center-of-mass"><a href="#center-of-mass" class="headerlink" title="center of mass"></a>center of mass</h5><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%主要参数</span></span><br><span class="line"><span class="comment">%center of mass 相对位置</span></span><br><span class="line">M01 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.05</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">M12 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.10</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">M23 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.10</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">M34 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.05</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">G = <span class="built_in">diag</span>(<span class="built_in">ones</span>(<span class="number">1</span>,<span class="number">6</span>)); <span class="comment">%inertia matrix</span></span><br><span class="line">w1 = [<span class="number">0</span>;<span class="number">0</span>;<span class="number">1</span>]; q1 = [<span class="number">0</span>;<span class="number">0</span>;<span class="number">0</span>]; q2 = [<span class="number">0.10</span>;<span class="number">0</span>;<span class="number">0</span>]; q3 = [<span class="number">0.20</span>;<span class="number">0</span>;<span class="number">0</span>];</span><br><span class="line">S = [[<span class="number">0</span>; <span class="number">0</span>; <span class="number">1</span>;    -<span class="built_in">cross</span>(w1,q1)], ...</span><br><span class="line">     [<span class="number">0</span>; <span class="number">0</span>; <span class="number">1</span>;    -<span class="built_in">cross</span>(w1,q2)], ...</span><br><span class="line">     [<span class="number">0</span>; <span class="number">0</span>; <span class="number">1</span>;    -<span class="built_in">cross</span>(w1,q3)]];</span><br></pre></td></tr></table></figure><!--  <s>我最初假设过长方体密度是1kg/m^3,但是惯性矩阵在matlab数量级在1e-5,太小了,导致结果与v-rep不一致,大概率是matlab舍入误差造成了,这一点可以加上format long试一下,记录完才想起来,就先留个位以后填吧.</s>我又用single rb测试了一下(毕竟单个刚体的更简单,可以直接写出方程),应该是Vrep软件舍入误差问题,我已经去论坛反馈了.- - 等回头再看看.  https://forum.coppeliarobotics.com/viewtopic.php?f=9&t=8356&p=32424#p32424 提完问我去逛了下手册,发现vrep里的 inertial matrix has divided by the mass. 所以是我MATLAB程序在计算cuboid惯性矩阵时除了问题,忘记给$I$部分乘上m了. 乌龙...--><blockquote><p>当inertia 矩阵非常小时, 欧拉积分不太合适,容易积累误差.</p></blockquote><h5 id="joint-frame"><a href="#joint-frame" class="headerlink" title="joint frame"></a>joint frame</h5><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%主要参数</span></span><br><span class="line"><span class="comment">%center of mass 相对位置</span></span><br><span class="line">M01 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.00</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">M12 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.10</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">M23 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.10</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">M34 = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">0.10</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]];</span><br><span class="line">G = <span class="built_in">diag</span>(<span class="built_in">ones</span>(<span class="number">1</span>,<span class="number">6</span>)); <span class="comment">%inertia matrix</span></span><br><span class="line">T = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>,  <span class="number">-0.05</span>]; [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]; [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br><span class="line">Ic = inertialmatirx_check(T,G) <span class="comment">%转化为Ic标架下</span></span><br><span class="line">w1 = [<span class="number">0</span>;<span class="number">0</span>;<span class="number">1</span>]; q1 = [<span class="number">0</span>;<span class="number">0</span>;<span class="number">0</span>]; q2 = [<span class="number">0.10</span>;<span class="number">0</span>;<span class="number">0</span>]; q3 = [<span class="number">0.20</span>;<span class="number">0</span>;<span class="number">0</span>];</span><br><span class="line">S = [[<span class="number">0</span>; <span class="number">0</span>; <span class="number">1</span>;    -<span class="built_in">cross</span>(w1,q1)], ...</span><br><span class="line">     [<span class="number">0</span>; <span class="number">0</span>; <span class="number">1</span>;    -<span class="built_in">cross</span>(w1,q2)], ...</span><br><span class="line">     [<span class="number">0</span>; <span class="number">0</span>; <span class="number">1</span>;    -<span class="built_in">cross</span>(w1,q3)]];</span><br></pre></td></tr></table></figure><h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><ul><li><p>Matlab<br><img src="testAnimated.gif" alt=""></p></li><li><p>V-rep<br><img src="vrep.gif" alt=""></p></li><li><p>对比.在Matlab中,步长取了50ms,与Vrep中一致,在仿真软件中选了最新的物理引擎.<br><img src="file.jpg" alt=""></p></li></ul><h5 id="下一步将继续研究离散的算法-待续"><a href="#下一步将继续研究离散的算法-待续" class="headerlink" title="下一步将继续研究离散的算法.(待续)"></a>下一步将继续研究离散的算法.(待续)</h5><hr><h4 id="附-V-rep基础教程"><a href="#附-V-rep基础教程" class="headerlink" title="附:V-rep基础教程"></a>附:V-rep基础教程</h4><p> 推荐阅读顺序如下:</p><ul><li><a href="https://www.coppeliarobotics.com/helpFiles/en/buildingAModelTutorial.htm" target="_blank" rel="noopener">Building a clean model tutorial</a> 从导入模型,建立关节开始大致熟悉一下流程,然后再用问题驱动,阅读User Manual最佳.</li><li><a href="https://www.coppeliarobotics.com/helpFiles/" target="_blank" rel="noopener">CoppeliaSim User Manual</a></li><li>最后,在Youtube有不少优秀的视频,例如<a href="https://www.youtube.com/watch?v=jfUA2W-niIc" target="_blank" rel="noopener">从CAD软件中导出Vrep读取的urdf文件并通过child Script控制</a><blockquote><p>这款仿真软件号称是机器人方面的瑞士军刀,在验证开发的算法以及可视化上很给力,并且可以通过API与Matlab通信,目前我只关注到这一层面,算法的研究上我还是用Matlab矩阵运算多一些,慢慢的结合V-rep做一些核对工作(<s>有时间有必要了解下它内部物理引擎所采取的算法,也先留个位吧</s>-.-)</p></blockquote></li></ul><h5 id="Bullet"><a href="#Bullet" class="headerlink" title="Bullet"></a>Bullet</h5><p>Bullet Physics is a professional <b>open source collision detection, rigid body and soft body dynamics<br>library</b>. The library is free for commercial use under the ZLib license. 在碰撞检测中用了GJK算法,室友以前还研究过,haha..巧的是,对于逆动力学算法,这个库采用的也是 Recursive Newton-Euler Algorithm (RNEA),而且<a href="https://pybullet.org/Bullet/phpBB3/viewtopic.php?t=12437" target="_blank" rel="noopener">The computation of the joint space inertia (mass) matrix  is using the Composite Rigid Body Algorithm</a>.与我在上述实验中采用的算法一样,因为是开源的(它的积分策略为Symplectic Euler integrator),我们有必要也关注下它的<a href="https://github.com/erwincoumans/bullet3/tree/master/src/BulletInverseDynamics" target="_blank" rel="noopener">Github 仓库</a>.<!--MultiBodyTree::MultiBodyImpl::calculateMassMatrix --></p><ul><li><a href="http://www.cs.kent.edu/~ruttan/GameEngines/lectures/Bullet_User_Manual" target="_blank" rel="noopener">user_Manual</a></li></ul><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;以平面RRR三连杆和空间三连杆为例,通过选择两种标架(joint frame以及center of mass frame),实现
      
    
    </summary>
    
    
      <category term="动力学" scheme="https://5imcs.com/categories/%E5%8A%A8%E5%8A%9B%E5%AD%A6/"/>
    
    
      <category term="仿真" scheme="https://5imcs.com/tags/%E4%BB%BF%E7%9C%9F/"/>
    
      <category term="Matlab &amp; Vrep" scheme="https://5imcs.com/tags/Matlab-Vrep/"/>
    
  </entry>
  
  <entry>
    <title>读论文real time obstacle avoidance for manipulators and moile robots并仿真</title>
    <link href="https://5imcs.com/posts/81ffa1cf/"/>
    <id>https://5imcs.com/posts/81ffa1cf/</id>
    <published>2020-02-20T04:04:39.000Z</published>
    <updated>2020-03-17T14:07:50.220Z</updated>
    
    <content type="html"><![CDATA[<h5 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h5><ul><li>精读论文并简单实现论文的例子</li><li>搭建一个基本的框架(包含正运动学,jacobian,正逆动力学部分),将其仿真实现,并可应用于n-dof的机械臂.</li></ul><p>仿真如下:(t=0.01,红色圆表示安全距离,蓝色圆为障碍.*点为目标点)<br><img src="artificial_potential.gif" alt=""></p><p>解决规划问题的论文有不少,但最经典的应该还是khatib1986那篇文章:real time obstacle avoidance for manipulators and moile robots.总的说来就是在operation space计算控制律,并通过jacobian矩阵将其转化为力矩,求得joint space的控制力作为控制信号.有陷入局部极小点的问题(这一块我将放在mathematica中进行分析,这款软件可视化很给力,尽管我还是个初学者)</p><h5 id="大体思路如下"><a href="#大体思路如下" class="headerlink" title="大体思路如下:"></a>大体思路如下:</h5><ol><li>寻找each arm segment距离障碍最近点,如果满足一定条件,则施加斥力</li><li>将斥力转化为关节力矩</li><li>作为控制律</li></ol><p>我们希望整个机械臂避免碰撞,但目前我们将问题减少至仅关心每段到障碍物最近的点.如果达到距离阀值则将施加斥力使其远离障碍.</p><p>为了找到手臂距离障碍物最近点$p_{psp}$,假设$x_c(:,i)$和$x_c(:,i+1)$为手臂(考虑直线)首尾端点.$ob_{orgin}$为障碍物中心.</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 计算各杆距离障碍物最近点closest,dist到障碍物中心距离,drhodx最近点指向障碍物外点向量</span></span><br><span class="line">taurep = <span class="built_in">zeros</span>(n,<span class="number">1</span>); <span class="comment">%斥力产生的力矩</span></span><br><span class="line">closest = []; dist = []; drhodx = [];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: n</span><br><span class="line">    vec_line = xc(:,<span class="built_in">i</span>+<span class="number">1</span>) - xc(:,<span class="built_in">i</span>);<span class="comment">%每一节端点</span></span><br><span class="line">    vec_ob_line = ob_orgin - xc(:,<span class="built_in">i</span>); <span class="comment">%the vector from the obstacle to the first line point</span></span><br><span class="line">    projection = <span class="built_in">dot</span>(vec_line,vec_ob_line)/<span class="built_in">dot</span>(vec_line,vec_line);</span><br><span class="line"><span class="keyword">if</span> projection  &lt; <span class="number">0</span></span><br><span class="line">    pclosest = xc(:,<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">elseif</span> projection &gt; <span class="number">1</span></span><br><span class="line">    pclosest = xc(:,<span class="built_in">i</span>+<span class="number">1</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    pclosest = xc(:,<span class="built_in">i</span>) + projection*vec_line;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">closest = [closest,pclosest]; <span class="comment">%ok</span></span><br></pre></td></tr></table></figure><p>利用夹角公式即可.<br>论文中<br>距离函数设计如下(arm closer,response stronger): </p><blockquote><p>$$F_{psp}=\eta(\frac{1}{\rho}-\frac{1}{\rho_0})\frac{1}{\rho^2}\frac{\partial \rho}{\partial x}$$<br>其中$\rho$表示到障碍的距离,$\rho_0$为安全(threshold)距离.(这里有个问题:如何合适的确定距离阀值呢?-这需要从论文的证明角度进行考虑.) </p></blockquote><hr><p>关键在于第二部分,我采用的是机械臂的几何雅克比矩阵[1] (spatial velocity twist jacobian,为方便表述记为$J_s$),故与论文中的$^0J$相差一个变换.</p><blockquote><p>几本教材的说法不太统一.</p></blockquote><p>为了计算受势场力点的jacobian矩阵,有三种方法,以三连杆为例. </p><p>第一种见:<a href="http://campus.unibo.it/218782/19/FIR_04_Kinem.pdf" target="_blank" rel="noopener">常见</a></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% 方法1 求得world frame EE velocity jacobian</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>: n</span><br><span class="line">    omg = p(:,n+<span class="number">1</span>)-p(:,<span class="built_in">i</span>);</span><br><span class="line">    matrix = [<span class="number">0</span>, -omg(<span class="number">3</span>), omg(<span class="number">2</span>); omg(<span class="number">3</span>), <span class="number">0</span>, -omg(<span class="number">1</span>); -omg(<span class="number">2</span>), omg(<span class="number">1</span>), <span class="number">0</span>];</span><br><span class="line">    js(<span class="number">4</span>:<span class="number">6</span>,<span class="built_in">i</span>) = -matrix*Slist(<span class="number">1</span>:<span class="number">3</span>,<span class="built_in">i</span>);</span><br><span class="line">    js(<span class="number">1</span>:<span class="number">3</span>,<span class="built_in">i</span>) = Slist(<span class="number">1</span>:<span class="number">3</span>,<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>第二种为数值计算:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%M为相对位姿&#123;psp&#125; in frame&#123;0&#125;</span><br><span class="line">epsilon &#x3D; 1e-3; </span><br><span class="line">epsilon_inv &#x3D; 1&#x2F;epsilon;</span><br><span class="line">n &#x3D; length(thetalist); % Dimension of the input x;</span><br><span class="line">ee &#x3D; fk(Slist,thetalist);</span><br><span class="line">f0 &#x3D; ee(1:3,4); </span><br><span class="line">% Do perturbation</span><br><span class="line">for i &#x3D; 1 : n</span><br><span class="line">    thetalist_ &#x3D; thetalist;</span><br><span class="line">    thetalist_(i) &#x3D;  thetalist(i) + epsilon;</span><br><span class="line">    ee_ &#x3D; fk(Slist,thetalist_);</span><br><span class="line">    jac(:, i) &#x3D; (ee_(1:3,4) - f0) .* epsilon_inv;</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>第三种为[1] 书中所介绍的,利用伴随映射和Twist计算.不再赘述.</p><blockquote><p>再利用$\tau = J^TM_{x}F_{psp}$计算关节空间的力矩,方便起见,仿真程序设$M_{x}$为单位矩阵.($M$ works to linearize the control)</p></blockquote><hr><p>特别注意的是,虚拟引力的设计,为了使得追踪过程end-effector走直线(不受虚拟斥力时),考虑速度限制,详细见论文.</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dx = js*dthetalist; </span><br><span class="line">lamb = kp/kv; vmax = <span class="number">2.5</span>; <span class="comment">%x_tilde = xc(:,end)-xd;</span></span><br><span class="line">dxd = lamb*(xd - xc(:,<span class="keyword">end</span>));v = vmax/(<span class="built_in">sqrt</span>(dxd'*dxd));</span><br><span class="line"><span class="keyword">if</span> v &gt; <span class="number">1</span></span><br><span class="line">    v = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">f_att = [<span class="built_in">zeros</span>(<span class="number">3</span>,<span class="number">1</span>);-kv*(dx(<span class="number">4</span>:<span class="number">6</span>,<span class="number">1</span>)-v*dxd)];</span><br></pre></td></tr></table></figure><p>关于动力学,采用递推公式(相比Lagrange方法,更适合多自由度情况),我将放在另一篇博客介绍.<a href="https://5imcs.com/posts/1e752288/">链接</a></p><p>[1]: A Mathematical Introduction to Robotic Manipulation</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h5 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;精读论文并简单实现论文的例子&lt;/li&gt;
&lt;li&gt;搭建一个基本的框架(包含正运动学,jacobian,正逆动力学部分),将其仿真实
      
    
    </summary>
    
    
      <category term="trajectory planning" scheme="https://5imcs.com/categories/trajectory-planning/"/>
    
    
      <category term="Matlab" scheme="https://5imcs.com/tags/Matlab/"/>
    
      <category term="仿真" scheme="https://5imcs.com/tags/%E4%BB%BF%E7%9C%9F/"/>
    
  </entry>
  
  <entry>
    <title>基于POE公式的机械臂正逆运动学仿真(Mathematica篇)</title>
    <link href="https://5imcs.com/posts/f7cdd882/"/>
    <id>https://5imcs.com/posts/f7cdd882/</id>
    <published>2020-02-20T04:04:39.000Z</published>
    <updated>2020-03-17T14:09:12.320Z</updated>
    
    <content type="html"><![CDATA[<h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><ul><li>记录cad的一些基本操作,并且结合Mathematica进行仿真环境搭建.具体包括</li></ul><ol><li>装配机械臂</li><li>在仿真环境中导入机械臂三维模型,并且进行正逆运动学仿真.</li><li>分析经典数值算法计算效率,并思考一些可改进的方向.</li></ol><h4 id="移动蛇臂建模"><a href="#移动蛇臂建模" class="headerlink" title="移动蛇臂建模"></a>移动蛇臂建模</h4><h5 id="smiley-由FreeCAD进行零件装配并建立正逆运动学模型-2020年2-1-2-5日"><a href="#smiley-由FreeCAD进行零件装配并建立正逆运动学模型-2020年2-1-2-5日" class="headerlink" title=":smiley:由FreeCAD进行零件装配并建立正逆运动学模型(2020年2.1-2.5日)"></a>:smiley:由FreeCAD进行零件装配并建立正逆运动学模型(2020年2.1-2.5日)</h5><ul><li>考虑joint limit(仅考虑了Joint space-&gt;configuration space)</li></ul><p>仿真结果如下:</p><p><img src="exampleRobot_frame.gif" alt=""></p><p>相关参考文献:</p><ul><li><p>1.1995.a weighted least-norm solution based scheme for avoiding joint limits for redundant joint manipulators.IEEE trans on robotics and automation.</p></li><li><p>2.2018.[文件](A Weighted Gradient Projection Method for Inverse Kinematics of Redundant Manipulators Considering Multiple Performance Criteria.pdf) . </p></li></ul><blockquote><p>思路为对 $\dot{q}=J^{-1}\dot{x}$中 $J$ 施加权重. 缺点:WLN或WGP方法在某些情况下无解.</p></blockquote><h5 id="可改进的地方"><a href="#可改进的地方" class="headerlink" title="可改进的地方:"></a>可改进的地方:</h5><ul><li>从反应规划(避障)角度考虑joint limit问题</li><li>从连续的角度近似超冗余,提高计算效率</li></ul><blockquote><p>后续将整理freecad的基本操作,以及相关公式和mma代码.  仿真的想法来源于robinvista的<a href="https://blog.csdn.net/robinvista/article/details/70231205" target="_blank" rel="noopener" title="基于Mathematica的机器人仿真环境（机械臂篇）">博客文章</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;记录cad的一些基本操作,并且结合Mathematica进行仿真环境搭建.具体包括&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;装配
      
    
    </summary>
    
    
      <category term="正逆运动学" scheme="https://5imcs.com/categories/%E6%AD%A3%E9%80%86%E8%BF%90%E5%8A%A8%E5%AD%A6/"/>
    
    
      <category term="仿真" scheme="https://5imcs.com/tags/%E4%BB%BF%E7%9C%9F/"/>
    
      <category term="Mathematica" scheme="https://5imcs.com/tags/Mathematica/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu16.04 软件安装</title>
    <link href="https://5imcs.com/posts/a287a6a9/"/>
    <id>https://5imcs.com/posts/a287a6a9/</id>
    <published>2020-02-19T12:38:55.947Z</published>
    <updated>2020-02-19T12:38:55.955Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Guake、git、shadowsocks、haroopad以及打印机、SSH"><a href="#Guake、git、shadowsocks、haroopad以及打印机、SSH" class="headerlink" title="Guake、git、shadowsocks、haroopad以及打印机、SSH"></a>Guake、git、shadowsocks、haroopad以及打印机、SSH</h4><a id="more"></a><p><s>1. shadowsocks</p><p>因为自己搭建了ss的缘故，为了下载各种包方便，我先给自己安装shadowsocks</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang&#x2F;ss-qt5</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shadowsocks-qt5</span><br></pre></td></tr></table></figure><p>安装完毕可以在dash中搜索并打开，输入ShadowSocks服务器的信息即可. 连上之后,我们还不能⻢上上网,还要设置 Ubuntu 的网络代理。点击系统设置,选择网络,接着选择网络代理,然后<code>设置手动(Manual), Socks Host:127.0.0.1 port:1080.</code></p><p>在百度中搜索ip，发现在us，说明这个ss设置的是==全局代理==，pac代理懒得设置了. </p><p>参考一下@[Ubuntu 16.04 使用 ShadowSocks + Privoxy 科学上网](Ubuntu 16.04 使用 ShadowSocks + Privoxy 科学上网) </s></p><blockquote><p>现在已改为用v2ray,通过<code>service v2ray start</code>启动</p></blockquote><ol start="2"><li>Guake<br>这款终端太赞了，可以半透明，占用内存小，按照&lt;完美应用ubuntu第三版&gt;来配置就可以了(设置开机启动). 此外也可以按照书中的指导配置<code>git</code>，并且push 本地仓库到github. 另外一个有意思的操作就是<a href="http://blog.csdn.net/lmj623565791/article/details/51319147" target="_blank" rel="noopener" title="如何利用github打造博客专属域名">如何利用github打造博客专属域名</a>,有兴趣可以看看. </li></ol><p><s>3. haroopad<br><code>Haroopad</code>我下载的是<code>haroopad-v0.13.1-x64.tar.gz</code>，挺好用，解压完运行，点击文件-偏好设置可以设置<code>math公式</code>和主题.</s></p><blockquote><p>改为用boostnote,记笔记更方便 `sudo apt-get install boostnote’</p></blockquote><ol start="4"><li>打印机<br>我的打印机是hp2132的，没有官方的linux驱动，所以我在google上找到了一个linux版本的第三方驱动<code>hplip-3.17.10.run</code>，这个驱动已经安装基本支持大部分打印机了，很给力.</li></ol><p>参考<code>HP Developer Portal _ Installer Walkthrough.pdf</code>，安装的时候出现了一个问题就是无法解决软件依赖，所以我设置了ubuntu<code>软件源为主服务器</code>，并且enable了互联网下载<code>universe和multiuniverse</code>，<strong>关闭了google/stable</strong>.</p><ol start="5"><li>SSH<br>因为经常需要维护自己的梯子，所以在ubuntu上也<code>sudo apt-get install openssh-client</code>安装一个ssh client,用来登录ssh server</li></ol><p>参考<code>man ssh</code>，通过shell命令<code>ssh root@104.224.162.206 -p 27616</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ben@ben:~$ ssh</span><br><span class="line">usage: ssh [-1246AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]</span><br><span class="line">           [-D [bind_address:]port] [-E log_file] [-e escape_char]</span><br><span class="line">           [-F configfile] [-I pkcs11] [-i identity_file] [-L address]</span><br><span class="line">           [-l login_name] [-m mac_spec] [-O ctl_cmd] [-o option] [-p port]</span><br><span class="line">           [-Q query_option] [-R address] [-S ctl_path] [-W host:port]</span><br><span class="line">           [-w local_tun[:remote_tun]] [user@]hostname [command]</span><br></pre></td></tr></table></figure><h4 id="Texlive2015与Texstudio"><a href="#Texlive2015与Texstudio" class="headerlink" title="Texlive2015与Texstudio"></a>Texlive2015与Texstudio</h4><ol><li>Texlive2015安装<br>因为windows 7下用的Texlive2015，很多文档基于此编译的</li><li>1 安装per组件<br><code>sudo apt-get install perl-tk</code></li></ol><blockquote><p>加载镜像组件（我挂在的是移动硬盘，到其目录下）<br><code>sudo mount -o loop texlive2015.iso /mnt</code></p></blockquote><blockquote><p>启动安装程序的图形化界面进行配置 (我默认安装在ssd上了)</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;mnt </span><br><span class="line">sudo .&#x2F;install-tl -gui</span><br></pre></td></tr></table></figure><blockquote><p>设置环境变量<br><code>sudo gedit ~/.bashrc</code></p></blockquote><p>==根据安装路径==末尾加上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export MANPATH&#x3D;$&#123;MANPATH&#125;:&#x2F;usr&#x2F;local&#x2F;texlive&#x2F;2015&#x2F;texmf-dist&#x2F;doc&#x2F;man</span><br><span class="line">export INFOPATH&#x3D;$&#123;INFOPATH&#125;:&#x2F;usr&#x2F;local&#x2F;texlive&#x2F;2015&#x2F;texmf-dist&#x2F;doc&#x2F;info</span><br><span class="line">export PATH&#x3D;$&#123;PATH&#125;:&#x2F;usr&#x2F;local&#x2F;texlive&#x2F;2015&#x2F;bin&#x2F;x86_64-linux</span><br></pre></td></tr></table></figure><ol start="2"><li>Texstudio<br>这款编辑器比较像winedt，所以我就直接在ubuntu 16.04软件中心直接安装了，需要注意的点在于如何配置</li></ol><p>TexStudio 中: option-&gt; configure texstudio-&gt; commands -&gt; latex/ pdflatex/ xelatex/ luralatex … 等到成功配置好这些命令的位置即可成功运行 TexStudio 并生成 pdf 文件了。</p><p>还存在的一个小问题是对于 eps 的图片格式会报错 XeLaTex xdvipdfmx:fatal: pdf_ref_obj(): passed invalid object ,而如果直接在终端的命令行中利用 xelatex 编译生成 pdf 文档则没有任何问题。解决的方法可以参考<a href="http://www.cnblogs.com/docnan/p/7103599.html?utm_source=itdadao&utm_medium=referral" target="_blank" rel="noopener" title="Latex加速:TexStudio的安装和使用-DocNan-博客园">Latex加速:TexStudio的安装和使用-DocNan-博客园</a>,也就是把 TexStudio ==在高级设置中的搜索路径直接替换成 TexLive 软件库中执行文件所在文件夹的路径==,在我的系统上就是: /usr/local/texlive/2015/bin/x86_64-linux ,这样设置之后果然没有任何问题了。估计还有一种可能性是经过这里搜索路径的更换,之前需要手动配置的 pdflatex, xelatex 等执行文件的位置也不用手动设了,但是此处 TexStudio 已经可以用了,就不做这个测试了。</p><p><del>@[图文参考pdf](file:/home/ben/下载/Latex加速_ TexStudio的安装和使用 - DocNan - 博客园.pdf)</del></p><ol start="3"><li>中文字体<br>CJK相关包中文支持<br><code>sudo apt-get install latex-cjk-chinese ttf-arphic-* hbf-*</code></li></ol><p>不过,为了得到更好地支持,常常会使用Windows既有的TrueType字体或者文泉驿字体,以便扩充现有的系统字体库。<br>对于Linux,安装fontforge可以方便地生成能被Latex识别的字体文件(map、enc等文件)<br><code>sudo apt-get install fontforge</code></p><p><a href="http://blog.csdn.net/bensnake/article/details/43279329" target="_blank" rel="noopener" title="Ubuntu14.04/14.10系统安装Latex及配置中文字体[修订]-CSDN博客">Ubuntu14.04/14.10系统安装Latex及配置中文字体[修订]-CSDN博客</a></p><ol start="4"><li>配置完遇到的一些问题<blockquote><p>毕业大论文texstudio打开中文乱码(从windows下拷贝到ubuntu)<br>gedit 打开拷贝到乱码的texstudio编辑器保存</p></blockquote></li></ol><blockquote><p>毕业论文beamer texstudio打开无乱码(从windows下拷贝到ubuntu)<br>猜想：或许由于windows下的beamer采用了utf-8编码(Encodings)，而4.1中windows下的则采用了gbk，建议采用utf-8</p></blockquote><h4 id="Mathematica11-0与MATLAB2015b安装"><a href="#Mathematica11-0与MATLAB2015b安装" class="headerlink" title="Mathematica11.0与MATLAB2015b安装"></a>Mathematica11.0与MATLAB2015b安装</h4><ol><li>Mathematica11.0<br>由于学校pt上只有version11.0的mathematica的包，所以我下载至移动硬盘，挂在至系统，进入sh文件目录</li><li>1 安装<br><code>sudo sh Mathematica.sh</code></li></ol><blockquote><p>激活<br>安装完毕，在dash中搜索mathematica打开，类似与windos激活即可。</p></blockquote><ol start="2"><li>Matlab R2015b_glnxa64.iso and Crack files<br>挂载iso至linux /mnt下<br><code>sudo mount -o loop R2015b_glnxa64.iso /mnt</code></li></ol><p>cd进2015b iso文件目录<br><code>sudo ./install</code></p><p>激活<br><a href="http://blog.csdn.net/hejunqing14/article/details/50265049" target="_blank" rel="noopener" title="linux(x64)下安装Matlab2015b破解版(含安装包)">linux(x64)下安装Matlab2015b破解版(含安装包)</a></p><p>运行<br><code>cd /usr/local/MATLAB/R2015b/bin</code></p><p><code>sudo ./matlab</code></p><h4 id="Eclipse-for-C-C-developer"><a href="#Eclipse-for-C-C-developer" class="headerlink" title="Eclipse for C/C++ developer"></a>Eclipse for C/C++ developer</h4><p>因为C语言的eclipse for c/c++ ide环境依赖于java环境，所以我们先安装这个，我已经在官网下载好并且放入移动硬盘了</p><ol><li>安装jdk环境<br>我下载的是jdk-8u151-linux-x64.tar.gz，将其放入/opt目录, 执行解压<br><code>tar -zxvf  jdk-8u151-linux-x64.tar.gz</code></li></ol><p>配置jdk的环境变量，打开 /etc/profile文件（sudo vim /etc/profile），在文件末尾添加下语句：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;jdk1.8.0_151</span><br><span class="line">export JRE_HOME&#x3D;$&#123;JAVA_HOME&#125;&#x2F;jre</span><br><span class="line">export CLASSPATH&#x3D;.:$&#123;JAVA_HOME&#125;&#x2F;lib:$&#123;JRE_HOME&#125;&#x2F;lib</span><br><span class="line">export PATH&#x3D;$&#123;JAVA_HOME&#125;&#x2F;bin:$PATH</span><br></pre></td></tr></table></figure><p>查看是否安装成功：<code>java -version</code> 出现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">java version &quot;1.8.0_151&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_151-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)</span><br></pre></td></tr></table></figure><ol start="2"><li>安装eclipse<br>我下载的是eclipse-cpp-neon-3-linux-gtk-x86_64.tar.gz,解压至/opt目录<br><code>sudo tar zxvf eclipse-jee-mars-2-linux-gtkeclipse-cpp-neon-3-linux-gtk-x86_64.tar.gz -C /opt/</code></li></ol><p>解压完可以运行，也可以参考<a href="http://blog.csdn.net/guolongpu/article/details/59501726" target="_blank" rel="noopener" title="ubuntu 搭建 eclipse c Ubuntu 16.04LTS中搭建Eclipse Neon2 [Eclipse IDE for C/C++ Developer]">ubuntu 搭建 eclipse c Ubuntu 16.04LTS中搭建Eclipse Neon2 [Eclipse IDE for C/C++ Developer]</a>先制作快捷启动图标. 这篇博客也教了大家怎么第一次使用eclipse for c来编译c程序, 测试安装是否到位.</p><p>我是直接cd 进目录直接运行再将其锁定到启动器的.</p><p>==作为一个新手我对与这种解压就可以运行的安装包tar.gz很好奇，前面都是apt-get，aptitude，deb,sh，iso之类的安装，后续研究下这类包的性质.==</p><h4 id="一些生产力工具"><a href="#一些生产力工具" class="headerlink" title="一些生产力工具"></a>一些生产力工具</h4><ol><li>MATLAB 工具箱exportfig_fig——————加速写作插图<br>写作是一件很有意思的事情，尤其在Latex上写作，插入美美的插图简直会中毒。首先，如果插入的是位图绝对不能忍的(别问为什么，强迫症)！！！而如Matlab print 保存的矢量图存在白边，且不能保留matlab中的patch的透明度。</li></ol><p>无意中在git上发现了一个工具包export_fig结合ghostscript\Xpdf工具，就可以解决Matlab保存图片白边过大，且不能保留矢量图透明度的问题。令人惊喜的是Linux中已经配置了gs, Xpdf(装了tex缘故)</p><p><code>gs -h</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GPL Ghostscript 9.18 (2015-10-05)</span><br><span class="line">Copyright (C) 2015 Artifex Software, Inc.  All rights reserved.</span><br></pre></td></tr></table></figure><p>将export_fig添加进路径,测试:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plot(sin(linspace(0, 10, 1000)), &#39;b:&#39;, &#39;LineWidth&#39;, 4);</span><br><span class="line">hold on</span><br><span class="line">plot(cos(linspace(0, 7, 1000)), &#39;r--&#39;, &#39;LineWidth&#39;, 3);</span><br><span class="line">grid on</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%export_fig test -eps</span><br></pre></td></tr></table></figure><h4 id="2-ImageMagick-图片互转-————-将gif转成png系列可以插入latex-beamer动画演示"><a href="#2-ImageMagick-图片互转-————-将gif转成png系列可以插入latex-beamer动画演示" class="headerlink" title="2.ImageMagick 图片互转 ————-将gif转成png系列可以插入latex beamer动画演示"></a>2.ImageMagick 图片互转 ————-将gif转成png系列可以插入latex beamer动画演示</h4><p>令人惊喜的是Linux中已经配置了ImageMagick(装了tex缘故)</p><p><code>convert cat.gif -coalesce cat_.png</code></p><hr><blockquote><p>配置大体就是这样了，主要用来写matlab/mathematica/c程序,以及通过terminal ssh管理raspberry网站服务器和web服务器.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Guake、git、shadowsocks、haroopad以及打印机、SSH&quot;&gt;&lt;a href=&quot;#Guake、git、shadowsocks、haroopad以及打印机、SSH&quot; class=&quot;headerlink&quot; title=&quot;Guake、git、shadowsocks、haroopad以及打印机、SSH&quot;&gt;&lt;/a&gt;Guake、git、shadowsocks、haroopad以及打印机、SSH&lt;/h4&gt;
    
    </summary>
    
    
      <category term="建站" scheme="https://5imcs.com/categories/%E5%BB%BA%E7%AB%99/"/>
    
    
      <category term="dev" scheme="https://5imcs.com/tags/dev/"/>
    
      <category term="ubuntu" scheme="https://5imcs.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>github pages 及hexo next博客搭建</title>
    <link href="https://5imcs.com/posts/1b71e0af/"/>
    <id>https://5imcs.com/posts/1b71e0af/</id>
    <published>2020-02-19T09:38:36.417Z</published>
    <updated>2020-02-19T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h4><a id="more"></a><ol><li>安装稳定新的node和npm版本,以及hexo<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">curl -o- https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;nvm-sh&#x2F;nvm&#x2F;v0.35.2&#x2F;install.sh | bash #安装nvm</span><br><span class="line"></span><br><span class="line">nvm ls-remote #检查远程仓库</span><br><span class="line">nvm install stable #安装稳定版本</span><br><span class="line">nvm use node #启用安装好的版本</span><br><span class="line"></span><br><span class="line"># check version</span><br><span class="line">node -v </span><br><span class="line">npm -v </span><br><span class="line"></span><br><span class="line">test node</span><br></pre></td></tr></table></figure><code>npm install hexo-cli -g</code><br>用于预览(可选)<br><code>npm install hexo-server -g</code></li></ol><ol start="2"><li>创建hexo site<br><code>cd /Data/workspace/git</code><br><code>mkdir blog</code><br><code>cd blog</code><br><code>hexo init</code></li></ol><blockquote><p>提示 You are almost done! Don’t forget to run ‘npm install’ before you start blogging with Hexo!</p></blockquote><p>  <code>npm install #安装依赖项</code><br>  <code>hexo server</code></p><blockquote><p>now we can visit localhost:4000 and create posts.</p></blockquote><h4 id="配置模版-喜欢next简洁风"><a href="#配置模版-喜欢next简洁风" class="headerlink" title="配置模版,喜欢next简洁风"></a>配置模版,喜欢next简洁风</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir themes&#x2F;next</span><br><span class="line">curl -s https:&#x2F;&#x2F;api.github.com&#x2F;repos&#x2F;iissnan&#x2F;hexo-theme-next&#x2F;releases&#x2F;latest | grep tarball_url | cut -d &#39;&quot;&#39; -f 4 | wget -i - -O- | tar -zx -C themes&#x2F;next --strip-components&#x3D;1</span><br></pre></td></tr></table></figure><h4 id="github"><a href="#github" class="headerlink" title="github"></a>github</h4><p><code>ssh-keygen -t rsa -C &#39;xxx@xxx.com</code><br><code>cat id_rsa.pub</code><br><code>ssh -T git@github.com #测试</code> </p><h4 id="deploy-to-github"><a href="#deploy-to-github" class="headerlink" title="deploy to github"></a>deploy to github</h4><p><code>npm install hexo-deployer-git --save #安装自动部署发布工具</code><br><code>npm install hexo-util -g</code><br><code>sudo gedit _config.yml</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">    type: git</span><br><span class="line">    repo:   git@github.com:benlw&#x2F;benlw.github.io.git</span><br><span class="line">    branch: master</span><br></pre></td></tr></table></figure><p> <code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d #发布</code></p><h4 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h4><p><code>ping benlw.github.io</code></p><blockquote><p>185.199.110.153</p></blockquote><p>最后,在仓库中设置CNAME内容<a href="https://5imcs.com">https://5imcs.com</a></p><h4 id="后续问题"><a href="#后续问题" class="headerlink" title="后续问题?"></a>后续问题?</h4><ol><li>Cannot Get /XXX</li></ol><blockquote><p>hexo new page “categories”</p></blockquote><p>2.<s>首页只显示标题,不显示摘要和内容<br>修改<code>\themes\next\layout_macro</code><br></s></p><ol start="3"><li><p>about页面,<code>hexo new page about</code>然后在source目录下就会生成对应的文件夹,每个文件夹里都有一个index.md，打开将页面的type设置为相应的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">title: 标签</span><br><span class="line">date: </span><br><span class="line">type: &quot;tags&quot; # 或者 &quot;categories&quot;&#x2F;&quot;about&quot;</span><br><span class="line">comments: false</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li><li><p>-待解决</p></li></ol><ul><li><input checked="" disabled="" type="checkbox"> 侧边栏连接失效<blockquote><p>next升级7.7</p></blockquote></li></ul><ol start="5"><li>在右上角或者左上角实现fork me on github,在这里选择喜欢的样式。这里选择的是</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href&#x3D;&quot;https:&#x2F;&#x2F;github.com&#x2F;benlw&#x2F;benlw.github.io&quot; class&#x3D;&quot;github-corner&quot; aria-label&#x3D;&quot;View source on Github&quot;&gt;&lt;svg width&#x3D;&quot;80&quot; height&#x3D;&quot;80&quot; viewBox&#x3D;&quot;0 0 250 250&quot; style&#x3D;&quot;fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;&quot; aria-hidden&#x3D;&quot;true&quot;&gt;&lt;path d&#x3D;&quot;M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z&quot;&gt;&lt;&#x2F;path&gt;&lt;path d&#x3D;&quot;M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2&quot; fill&#x3D;&quot;currentColor&quot; style&#x3D;&quot;transform-origin: 130px 106px;&quot; class&#x3D;&quot;octo-arm&quot;&gt;&lt;&#x2F;path&gt;&lt;path d&#x3D;&quot;M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z&quot; fill&#x3D;&quot;currentColor&quot; class&#x3D;&quot;octo-body&quot;&gt;&lt;&#x2F;path&gt;&lt;&#x2F;svg&gt;&lt;&#x2F;a&gt;&lt;style&gt;.github-corner:hover .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;@keyframes octocat-wave&#123;0%,100%&#123;transform:rotate(0)&#125;20%,60%&#123;transform:rotate(-25deg)&#125;40%,80%&#123;transform:rotate(10deg)&#125;&#125;@media (max-width:500px)&#123;.github-corner:hover .octo-arm&#123;animation:none&#125;.github-corner .octo-arm&#123;animation:octocat-wave 560ms ease-in-out&#125;&#125;&lt;&#x2F;style&gt;</span><br></pre></td></tr></table></figure><p>将其粘贴到themes/next/layout/_layout.swig中，放在div class=”headband”&gt;</div>下面</p><ol start="6"><li>公式测试</li></ol><p>$$ \tau = J^T F$$</p><ol start="7"><li>测试pdf<div class="pdf" target="/hexo/next-pdf/1.pdf" height=""></div><blockquote><p><a href="example.pdf">点我，这里是PDF文档</a></p></blockquote></li></ol><ol start="8"><li>测试图片<br>注: next7主题已经可以不用安装hexo的image插件了.直接打开</li></ol><p><code>[1](1.png) #不引用</code></p><p><img src="1.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% asset_img 1.png This is an image %&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;环境配置&quot;&gt;&lt;a href=&quot;#环境配置&quot; class=&quot;headerlink&quot; title=&quot;环境配置&quot;&gt;&lt;/a&gt;环境配置&lt;/h4&gt;
    
    </summary>
    
    
      <category term="建站" scheme="https://5imcs.com/categories/%E5%BB%BA%E7%AB%99/"/>
    
    
      <category term="web" scheme="https://5imcs.com/tags/web/"/>
    
      <category term="hexo" scheme="https://5imcs.com/tags/hexo/"/>
    
      <category term="github pages" scheme="https://5imcs.com/tags/github-pages/"/>
    
  </entry>
  
</feed>
